{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kayote/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# We'll need numpy for some mathematical operations\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# matplotlib for displaying the output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "# and IPython.display for audio output\n",
    "import IPython.display\n",
    "\n",
    "# Librosa for audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#audio_path = librosa.util.example_audio_file()\n",
    "audio_path = 'first_test2.aiff'\n",
    "\n",
    "y, sr = librosa.load(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.48661117593e+12\n",
      "413903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>bank</th>\n",
       "      <th>pitch</th>\n",
       "      <th>vel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.486611e+12</td>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.486611e+12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.486611e+12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.486611e+12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.486611e+12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ts   bank  pitch   vel\n",
       "0  1.486611e+12  start    NaN   NaN\n",
       "1  1.486611e+12      0    1.0  32.0\n",
       "2  1.486611e+12      0    1.0   0.0\n",
       "3  1.486611e+12      0    2.0  32.0\n",
       "4  1.486611e+12      0    2.0   0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('first_test2.txt', names=['ts', 'bank', 'pitch', 'vel'],sep=' ')\n",
    "chirp_ts = data.ts[data.bank=='start'].iloc[0]\n",
    "print(chirp_ts)\n",
    "chirp_ind = np.min(np.where(y>0.1))\n",
    "print(chirp_ind)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample(y,ts1,ts2,sr,chirp_ts,chirp_ind):\n",
    "    ind1 = int(chirp_ind+(ts1+150-chirp_ts)*sr/1000.0)\n",
    "    ind2 = int(chirp_ind+(ts2-150-chirp_ts)*sr/1000.0)\n",
    "    return y[ind1:ind2]\n",
    "\n",
    "def get_sample_by_meta(y,data,bank,pitch,vel,sr,chirp_ts,chirp_ind):\n",
    "    ts1 = data.ts[(data.pitch==pitch) & (data.bank==bank) & (data.vel==vel)].iloc[0]\n",
    "    ts2 = data.ts[np.where((data.pitch==pitch) & (data.bank==bank) & (data.vel==vel))[0]+1].iloc[0]\n",
    "    ind1 = int(chirp_ind+(ts1+150-chirp_ts)*sr/1000.0)\n",
    "    ind2 = int(chirp_ind+(ts2-150-chirp_ts)*sr/1000.0)\n",
    "    return y[ind1:ind2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'sample': [[]], 'vel': 32.0, 'bank': 'start', 'pitch': nan}\n",
      "    bank  pitch sample   vel\n",
      "0  start    NaN     []  32.0\n",
      "(485, 4)\n",
      "    bank  pitch                                             sample   vel\n",
      "0  start    NaN                                                 []  32.0\n",
      "1      0    1.0  [4.44706e-06, 9.13977e-07, -4.78962e-06, 9.225...  32.0\n",
      "2      1    1.0  [1.05107e-06, -8.39425e-07, -9.55803e-07, -4.7...  32.0\n",
      "3      0    1.0  [-3.14419e-06, 4.66695e-06, -5.74328e-06, 5.29...  64.0\n",
      "4      0    1.0  [-7.5734e-06, 4.44263e-06, -1.09487e-06, -2.57...  96.0\n"
     ]
    }
   ],
   "source": [
    "print(np.array(0))\n",
    "first_dict = {'bank':data.bank[0],'pitch':data.pitch[0],'vel':data.vel[9],'sample':[[]]}\n",
    "print(first_dict)\n",
    "samples = pd.DataFrame(first_dict,index=[0])\n",
    "print(samples)\n",
    "for pitch in np.unique(data.pitch):\n",
    "    for vel in np.setdiff1d(np.unique(data.vel),0):\n",
    "        for bank in np.setdiff1d(np.unique(data.bank),'start'):\n",
    "            if np.isnan(pitch): continue\n",
    "            if np.isnan(vel): continue\n",
    "            try:\n",
    "                sample = get_sample_by_meta(y,data,bank,int(pitch),int(vel),sr,chirp_ts,chirp_ind)\n",
    "                samples = samples.append({'bank':bank,'pitch':pitch,'vel':vel,'sample':sample},ignore_index=True)\n",
    "            except: continue\n",
    "                \n",
    "print(samples.shape)\n",
    "print(samples.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bank</th>\n",
       "      <th>pitch</th>\n",
       "      <th>sample</th>\n",
       "      <th>vel</th>\n",
       "      <th>fft</th>\n",
       "      <th>total_sample</th>\n",
       "      <th>valid_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[4.44706e-06, 9.13977e-07, -4.78962e-06, 9.225...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.053637</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.05107e-06, -8.39425e-07, -9.55803e-07, -4.7...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.052744</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-3.14419e-06, 4.66695e-06, -5.74328e-06, 5.29...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.052508</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-7.5734e-06, 4.44263e-06, -1.09487e-06, -2.57...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.053330</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bank  pitch                                             sample   vel fft  \\\n",
       "0  start    NaN                                                 []  32.0  []   \n",
       "1      0    1.0  [4.44706e-06, 9.13977e-07, -4.78962e-06, 9.225...  32.0  []   \n",
       "2      1    1.0  [1.05107e-06, -8.39425e-07, -9.55803e-07, -4.7...  32.0  []   \n",
       "3      0    1.0  [-3.14419e-06, 4.66695e-06, -5.74328e-06, 5.29...  64.0  []   \n",
       "4      0    1.0  [-7.5734e-06, 4.44263e-06, -1.09487e-06, -2.57...  96.0  []   \n",
       "\n",
       "   total_sample valid_sample  \n",
       "0      0.000000        False  \n",
       "1      0.053637        False  \n",
       "2      0.052744        False  \n",
       "3      0.052508        False  \n",
       "4      0.053330        False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIyMh8f9vFIyCUTAKRsEoGAVkAwCMjcPXnLM5lwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11eeb9210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_fft(sample):\n",
    "    try:\n",
    "        if np.sum(np.abs(sample))<1.0: return []\n",
    "        S_sample = np.abs((librosa.stft(sample)))**0.5\n",
    "        return np.abs(S_sample[:,S_sample.shape[1]/2])\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def get_fft_with_pitch_shift(sample,sr,n_steps=0):\n",
    "    if np.sum(np.abs(sample))<1.0: return []\n",
    "    S_sample = np.abs((librosa.stft(sample)))**0.5\n",
    "    S_sample = np.abs((librosa.stft(librosa.effects.pitch_shift(sample,sr,n_steps=n_steps))))**0.5\n",
    "    return np.abs(S_sample[:,S_sample.shape[1]/2])\n",
    "        \n",
    "        \n",
    "samples['fft'] = [get_fft(sample) for sample in samples['sample']]\n",
    "samples['total_sample'] = [sum(np.abs(sample)) for sample in samples['sample']]\n",
    "\n",
    "samples['valid_sample'] = [True if len(x)>0 else False for x in samples['fft']]\n",
    "plt.plot(samples['pitch'],samples['total_sample'])\n",
    "\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chord_pitches(root,majmin3='maj',majmin7='none',extensions=[]):\n",
    "    \n",
    "    fundamental = root%12\n",
    "    fundamentals = [fundamental + x*12 for x in range(12)]\n",
    "    \n",
    "    all_of_them = fundamentals\n",
    "    if majmin3 =='maj': all_of_them += [(fundamental+4)%12 + x*12 for x in range(12)]\n",
    "    if majmin3 =='min': all_of_them += [(fundamental+3)%12 + x*12 for x in range(12)]\n",
    "    all_of_them += [(fundamental+7)%12 + x*12 for x in range(12)]\n",
    "    if majmin7 =='maj': all_of_them += [(fundamental+11)%12 + x*12 for x in range(12)]\n",
    "    if majmin7 =='min': all_of_them += [(fundamental+10)%12 + x*12 for x in range(12)]\n",
    "    for extension in extensions:\n",
    "        if extension not in range(12): continue\n",
    "        all_of_them += [(fundamental+extension)%12 + x*12 for x in range(12)]\n",
    "        \n",
    "    return sorted([num for num in all_of_them if num >= 1 and num <= 127])\n",
    "\n",
    "def make_chord(root,samples,root_or_pitch_class='root',bank='0',vel=128,majmin3='maj',majmin7='none',extensions=[]):\n",
    "    if root_or_pitch_class=='root': \n",
    "        all_notes = get_chord_pitches(root,majmin3,majmin7,extensions)\n",
    "        all_notes = sorted([num for num in all_notes if num >= 40 and num <= 63])\n",
    "    else: \n",
    "        all_notes = [x + 12*y for x in root for y in range(12)]\n",
    "        all_notes = sorted([num for num in all_notes if num >= 40 and num <= 63])\n",
    "        #print(all_notes)\n",
    "    if len(all_notes)>0: pitch_class = list(frozenset([x%12 for x in all_notes]))[0]\n",
    "    else: \n",
    "        pitch_class = []\n",
    "        return None\n",
    "    pitch_class = list(frozenset([x%12 for x in all_notes]))[0]\n",
    "    weights = np.random.uniform(0.25,1,len(all_notes))\n",
    "    ever_worked = False\n",
    "    good_weights = []\n",
    "    ever_worked = False\n",
    "    for i in range(len(weights)):\n",
    "        tmp = samples['fft'][(samples['bank']==bank) & (samples['vel']==vel) & (samples['pitch']==all_notes[i])]\n",
    "        if len(tmp)==0: continue\n",
    "        tmp = tmp.iloc[0]\n",
    "        if len(tmp)==0: continue\n",
    "        if not ever_worked: \n",
    "            tmp2 = tmp\n",
    "            ever_worked = True\n",
    "            good_weights = [weights[i]]\n",
    "        else: \n",
    "            tmp2 = np.vstack((tmp2,tmp))\n",
    "            good_weights.append(weights[i])\n",
    "    fft = np.dot(good_weights,tmp2)\n",
    "    return fft/np.sum(fft)#, sample[0]/np.max(sample[0])\n",
    "\n",
    "\n",
    "def make_chord_with_pitch_shift(root,samples,sr,root_or_pitch_class='root',n_steps=0,bank='0',vel=128,majmin3='maj',majmin7='none',extensions=[]):\n",
    "    if root_or_pitch_class=='root': all_notes = get_chord_pitches(root,majmin3,majmin7,extensions)\n",
    "    else: \n",
    "        all_notes = [x + 12*y for x in root for y in range(12)]\n",
    "        all_notes = sorted([num for num in all_notes if num >= 1 and num <= 127])\n",
    "    pitch_class = list(frozenset([x%12 for x in all_notes]))[0]\n",
    "    weights = np.random.uniform(0.25,1,len(all_notes))\n",
    "    ever_worked = False\n",
    "    for i in range(len(weights)):\n",
    "        sample = samples['sample'][(samples['bank']==bank) & (samples['vel']==vel) & (samples['pitch']==all_notes[i])]\n",
    "        if len(sample)==0: continue\n",
    "        tmp = np.array(get_fft_with_pitch_shift(np.array(sample)[0],sr,n_steps))\n",
    "        if len(tmp)==0: continue\n",
    "        #tmp2 = np.array(samples['sample'][(samples['bank']==bank) & (samples['vel']==vel) & (samples['pitch']==all_notes[i])])\n",
    "        if not ever_worked:\n",
    "            fft = weights[i]*tmp\n",
    "            ever_worked = True\n",
    "            #sample = weights[i]*tmp2\n",
    "        else: \n",
    "            fft += weights[i]*tmp\n",
    "            #sample += weights[i]*tmp2\n",
    "    if ever_worked: return fft/np.sum(fft), pitch_class#, sample[0]/np.max(sample[0])\n",
    "    else: return []\n",
    "    \n",
    "def to1hot(row,num_categories=2):\n",
    "    one_hot = np.zeros(num_categories)\n",
    "    one_hot[row]=1.0\n",
    "    return one_hot\n",
    "\n",
    "def toMultiHot(pitch_bank,num_categories=12):\n",
    "    output = np.zeros(num_categories)\n",
    "    for i in pitch_bank:\n",
    "        output[i] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing zero-notes\n",
      "{'pitch_shift': 0, 'repetition': 0, 'fft': None, 'pitch_class': []}\n",
      "Doing one-notes\n",
      "Step 0 of 12 at 0 percent: \n",
      "Step 1 of 12 at 8.33333 percent: \n",
      "Step 2 of 12 at 16.6667 percent: \n",
      "Step 3 of 12 at 25 percent: \n",
      "Step 4 of 12 at 33.3333 percent: \n",
      "Step 5 of 12 at 41.6667 percent: \n",
      "Step 6 of 12 at 50 percent: \n",
      "Step 7 of 12 at 58.3333 percent: \n",
      "Step 8 of 12 at 66.6667 percent: \n",
      "Step 9 of 12 at 75 percent: \n",
      "Step 10 of 12 at 83.3333 percent: \n",
      "Step 11 of 12 at 91.6667 percent: \n",
      "Doing two-notes\n",
      "Step 0 of 144 at 0 percent: \n",
      "Step 1 of 144 at 0.694444 percent: \n",
      "Step 2 of 144 at 1.38889 percent: \n",
      "Step 3 of 144 at 2.08333 percent: \n",
      "Step 4 of 144 at 2.77778 percent: \n",
      "Step 5 of 144 at 3.47222 percent: \n",
      "Step 6 of 144 at 4.16667 percent: \n",
      "Step 7 of 144 at 4.86111 percent: \n",
      "Step 8 of 144 at 5.55556 percent: \n",
      "Step 9 of 144 at 6.25 percent: \n",
      "Step 10 of 144 at 6.94444 percent: \n",
      "Step 11 of 144 at 7.63889 percent: \n",
      "Step 12 of 144 at 8.33333 percent: \n",
      "Step 13 of 144 at 9.02778 percent: \n",
      "Step 14 of 144 at 9.72222 percent: \n",
      "Step 15 of 144 at 10.4167 percent: \n",
      "Step 16 of 144 at 11.1111 percent: \n",
      "Step 17 of 144 at 11.8056 percent: \n",
      "Step 18 of 144 at 12.5 percent: \n",
      "Step 19 of 144 at 13.1944 percent: \n",
      "Step 20 of 144 at 13.8889 percent: \n",
      "Step 21 of 144 at 14.5833 percent: \n",
      "Step 22 of 144 at 15.2778 percent: \n",
      "Step 23 of 144 at 15.9722 percent: \n",
      "Step 24 of 144 at 16.6667 percent: \n",
      "Step 25 of 144 at 17.3611 percent: \n",
      "Step 26 of 144 at 18.0556 percent: \n",
      "Step 27 of 144 at 18.75 percent: \n",
      "Step 28 of 144 at 19.4444 percent: \n",
      "Step 29 of 144 at 20.1389 percent: \n",
      "Step 30 of 144 at 20.8333 percent: \n",
      "Step 31 of 144 at 21.5278 percent: \n",
      "Step 32 of 144 at 22.2222 percent: \n",
      "Step 33 of 144 at 22.9167 percent: \n",
      "Step 34 of 144 at 23.6111 percent: \n",
      "Step 35 of 144 at 24.3056 percent: \n",
      "Step 36 of 144 at 25 percent: \n",
      "Step 37 of 144 at 25.6944 percent: \n",
      "Step 38 of 144 at 26.3889 percent: \n",
      "Step 39 of 144 at 27.0833 percent: \n",
      "Step 40 of 144 at 27.7778 percent: \n",
      "Step 41 of 144 at 28.4722 percent: \n",
      "Step 42 of 144 at 29.1667 percent: \n",
      "Step 43 of 144 at 29.8611 percent: \n",
      "Step 44 of 144 at 30.5556 percent: \n",
      "Step 45 of 144 at 31.25 percent: \n",
      "Step 46 of 144 at 31.9444 percent: \n",
      "Step 47 of 144 at 32.6389 percent: \n",
      "Step 48 of 144 at 33.3333 percent: \n",
      "Step 49 of 144 at 34.0278 percent: \n",
      "Step 50 of 144 at 34.7222 percent: \n",
      "Step 51 of 144 at 35.4167 percent: \n",
      "Step 52 of 144 at 36.1111 percent: \n",
      "Step 53 of 144 at 36.8056 percent: \n",
      "Step 54 of 144 at 37.5 percent: \n",
      "Step 55 of 144 at 38.1944 percent: \n",
      "Step 56 of 144 at 38.8889 percent: \n",
      "Step 57 of 144 at 39.5833 percent: \n",
      "Step 58 of 144 at 40.2778 percent: \n",
      "Step 59 of 144 at 40.9722 percent: \n",
      "Step 60 of 144 at 41.6667 percent: \n",
      "Step 61 of 144 at 42.3611 percent: \n",
      "Step 62 of 144 at 43.0556 percent: \n",
      "Step 63 of 144 at 43.75 percent: \n",
      "Step 64 of 144 at 44.4444 percent: \n",
      "Step 65 of 144 at 45.1389 percent: \n",
      "Step 66 of 144 at 45.8333 percent: \n",
      "Step 67 of 144 at 46.5278 percent: \n",
      "Step 68 of 144 at 47.2222 percent: \n",
      "Step 69 of 144 at 47.9167 percent: \n",
      "Step 70 of 144 at 48.6111 percent: \n",
      "Step 71 of 144 at 49.3056 percent: \n",
      "Step 72 of 144 at 50 percent: \n",
      "Step 73 of 144 at 50.6944 percent: \n",
      "Step 74 of 144 at 51.3889 percent: \n",
      "Step 75 of 144 at 52.0833 percent: \n",
      "Step 76 of 144 at 52.7778 percent: \n",
      "Step 77 of 144 at 53.4722 percent: \n",
      "Step 78 of 144 at 54.1667 percent: \n",
      "Step 79 of 144 at 54.8611 percent: \n",
      "Step 80 of 144 at 55.5556 percent: \n",
      "Step 81 of 144 at 56.25 percent: \n",
      "Step 82 of 144 at 56.9444 percent: \n",
      "Step 83 of 144 at 57.6389 percent: \n",
      "Step 84 of 144 at 58.3333 percent: \n",
      "Step 85 of 144 at 59.0278 percent: \n",
      "Step 86 of 144 at 59.7222 percent: \n",
      "Step 87 of 144 at 60.4167 percent: \n",
      "Step 88 of 144 at 61.1111 percent: \n",
      "Step 89 of 144 at 61.8056 percent: \n",
      "Step 90 of 144 at 62.5 percent: \n",
      "Step 91 of 144 at 63.1944 percent: \n",
      "Step 92 of 144 at 63.8889 percent: \n",
      "Step 93 of 144 at 64.5833 percent: \n",
      "Step 94 of 144 at 65.2778 percent: \n",
      "Step 95 of 144 at 65.9722 percent: \n",
      "Step 96 of 144 at 66.6667 percent: \n",
      "Step 97 of 144 at 67.3611 percent: \n",
      "Step 98 of 144 at 68.0556 percent: \n",
      "Step 99 of 144 at 68.75 percent: \n",
      "Step 100 of 144 at 69.4444 percent: \n",
      "Step 101 of 144 at 70.1389 percent: \n",
      "Step 102 of 144 at 70.8333 percent: \n",
      "Step 103 of 144 at 71.5278 percent: \n",
      "Step 104 of 144 at 72.2222 percent: \n",
      "Step 105 of 144 at 72.9167 percent: \n",
      "Step 106 of 144 at 73.6111 percent: \n",
      "Step 107 of 144 at 74.3056 percent: \n",
      "Step 108 of 144 at 75 percent: \n",
      "Step 109 of 144 at 75.6944 percent: \n",
      "Step 110 of 144 at 76.3889 percent: \n",
      "Step 111 of 144 at 77.0833 percent: \n",
      "Step 112 of 144 at 77.7778 percent: \n",
      "Step 113 of 144 at 78.4722 percent: \n",
      "Step 114 of 144 at 79.1667 percent: \n",
      "Step 115 of 144 at 79.8611 percent: \n",
      "Step 116 of 144 at 80.5556 percent: \n",
      "Step 117 of 144 at 81.25 percent: \n",
      "Step 118 of 144 at 81.9444 percent: \n",
      "Step 119 of 144 at 82.6389 percent: \n",
      "Step 120 of 144 at 83.3333 percent: \n",
      "Step 121 of 144 at 84.0278 percent: \n",
      "Step 122 of 144 at 84.7222 percent: \n",
      "Step 123 of 144 at 85.4167 percent: \n",
      "Step 124 of 144 at 86.1111 percent: \n",
      "Step 125 of 144 at 86.8056 percent: \n",
      "Step 126 of 144 at 87.5 percent: \n",
      "Step 127 of 144 at 88.1944 percent: \n",
      "Step 128 of 144 at 88.8889 percent: \n",
      "Step 129 of 144 at 89.5833 percent: \n",
      "Step 130 of 144 at 90.2778 percent: \n",
      "Step 131 of 144 at 90.9722 percent: \n",
      "Step 132 of 144 at 91.6667 percent: \n",
      "Step 133 of 144 at 92.3611 percent: \n",
      "Step 134 of 144 at 93.0556 percent: \n",
      "Step 135 of 144 at 93.75 percent: \n",
      "Step 136 of 144 at 94.4444 percent: \n",
      "Step 137 of 144 at 95.1389 percent: \n",
      "Step 138 of 144 at 95.8333 percent: \n",
      "Step 139 of 144 at 96.5278 percent: \n",
      "Step 140 of 144 at 97.2222 percent: \n",
      "Step 141 of 144 at 97.9167 percent: \n",
      "Step 142 of 144 at 98.6111 percent: \n",
      "Step 143 of 144 at 99.3056 percent: \n",
      "51.4573152065\n"
     ]
    }
   ],
   "source": [
    "def tmp2(j):\n",
    "    n_steps = 0\n",
    "    chord_fft = make_chord(pitch_class,samples,root_or_pitch_class='pitch_class')\n",
    "    #n_steps = np.random.uniform(-0.1,0.1,1)\n",
    "    #chord_fft = make_chord_with_pitch_shift(pitch_class,samples,sr,majmin3=label_to_chord[i]['majmin3'],majmin7=label_to_chord[i]['majmin7'],n_steps=n_steps)\n",
    "    tmp_dict = {'fft':chord_fft,'pitch_shift':n_steps,'pitch_class':pitch_class}\n",
    "    tmp_dict.update({'repetition':j})\n",
    "    return tmp_dict\n",
    "\n",
    "num_reps = num_cores*4\n",
    "ds = pd.DataFrame()\n",
    "#parallel_output = Parallel(n_jobs=num_cores)(delayed(tmp2)(i) for i in range(num_reps))\n",
    "#print(parallel_output[0])\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#zero notes\n",
    "print('Doing zero-notes')\n",
    "pitch_class = []\n",
    "print(tmp2(0))\n",
    "parallel_output = Parallel(n_jobs=num_cores)(delayed(tmp2)(i) for i in range(num_reps*20))\n",
    "ds = ds.append(parallel_output,ignore_index=True)\n",
    "\n",
    "#one notes\n",
    "print('Doing one-notes')\n",
    "for i1 in range(12):\n",
    "    print(\"Step %g of %g at %g percent: \"%(i1,12,100*(i1)/(12.)))\n",
    "    pitch_class = [i1]\n",
    "    parallel_output = Parallel(n_jobs=num_cores)(delayed(tmp2)(i) for i in range(num_reps*4))\n",
    "    ds = ds.append(parallel_output,ignore_index=True)\n",
    "\n",
    "print('Doing two-notes')\n",
    "for i1 in range(12):\n",
    "    for i2 in range(12):\n",
    "        print(\"Step %g of %g at %g percent: \"%(i1*12+i2,12*12,100*(i1*12+i2)/(12*12.)))\n",
    "        pitch_class = [i1, i2]\n",
    "        parallel_output = Parallel(n_jobs=num_cores)(delayed(tmp2)(i) for i in range(num_reps*2))\n",
    "        ds = ds.append(parallel_output,ignore_index=True)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(stop-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 4, 7], 'root': 0, 'extensions': 'none'}\n",
      "Step 1 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 5, 8], 'root': 1, 'extensions': 'none'}\n",
      "Step 2 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 6, 9], 'root': 2, 'extensions': 'none'}\n",
      "Step 3 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 7, 10], 'root': 3, 'extensions': 'none'}\n",
      "Step 4 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [4, 8, 11], 'root': 4, 'extensions': 'none'}\n",
      "Step 5 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 5, 9], 'root': 5, 'extensions': 'none'}\n",
      "Step 6 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 6, 10], 'root': 6, 'extensions': 'none'}\n",
      "Step 7 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 7, 11], 'root': 7, 'extensions': 'none'}\n",
      "Step 8 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 3, 8], 'root': 8, 'extensions': 'none'}\n",
      "Step 9 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 4, 9], 'root': 9, 'extensions': 'none'}\n",
      "Step 10 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 5, 10], 'root': 10, 'extensions': 'none'}\n",
      "Step 11 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 6, 11], 'root': 11, 'extensions': 'none'}\n",
      "Step 12 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 7], 'root': 0, 'extensions': 'none'}\n",
      "Step 13 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 8], 'root': 1, 'extensions': 'none'}\n",
      "Step 14 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 5, 9], 'root': 2, 'extensions': 'none'}\n",
      "Step 15 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 6, 10], 'root': 3, 'extensions': 'none'}\n",
      "Step 16 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [4, 7, 11], 'root': 4, 'extensions': 'none'}\n",
      "Step 17 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 5, 8], 'root': 5, 'extensions': 'none'}\n",
      "Step 18 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 6, 9], 'root': 6, 'extensions': 'none'}\n",
      "Step 19 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 7, 10], 'root': 7, 'extensions': 'none'}\n",
      "Step 20 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 8, 11], 'root': 8, 'extensions': 'none'}\n",
      "Step 21 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 4, 9], 'root': 9, 'extensions': 'none'}\n",
      "Step 22 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 5, 10], 'root': 10, 'extensions': 'none'}\n",
      "Step 23 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 6, 11], 'root': 11, 'extensions': 'none'}\n",
      "Step 24 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 7, 11], 'root': 0, 'extensions': 'none'}\n",
      "Step 25 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 8], 'root': 1, 'extensions': 'none'}\n",
      "Step 26 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 9], 'root': 2, 'extensions': 'none'}\n",
      "Step 27 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 7, 10], 'root': 3, 'extensions': 'none'}\n",
      "Step 28 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 4, 8, 11], 'root': 4, 'extensions': 'none'}\n",
      "Step 29 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 9], 'root': 5, 'extensions': 'none'}\n",
      "Step 30 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 10], 'root': 6, 'extensions': 'none'}\n",
      "Step 31 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 11], 'root': 7, 'extensions': 'none'}\n",
      "Step 32 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 3, 7, 8], 'root': 8, 'extensions': 'none'}\n",
      "Step 33 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 4, 8, 9], 'root': 9, 'extensions': 'none'}\n",
      "Step 34 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 5, 9, 10], 'root': 10, 'extensions': 'none'}\n",
      "Step 35 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 6, 10, 11], 'root': 11, 'extensions': 'none'}\n",
      "Step 36 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 7, 11], 'root': 0, 'extensions': 'none'}\n",
      "Step 37 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 8], 'root': 1, 'extensions': 'none'}\n",
      "Step 38 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 9], 'root': 2, 'extensions': 'none'}\n",
      "Step 39 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 6, 10], 'root': 3, 'extensions': 'none'}\n",
      "Step 40 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 4, 7, 11], 'root': 4, 'extensions': 'none'}\n",
      "Step 41 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 8], 'root': 5, 'extensions': 'none'}\n",
      "Step 42 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 9], 'root': 6, 'extensions': 'none'}\n",
      "Step 43 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 10], 'root': 7, 'extensions': 'none'}\n",
      "Step 44 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 7, 8, 11], 'root': 8, 'extensions': 'none'}\n",
      "Step 45 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 8, 9], 'root': 9, 'extensions': 'none'}\n",
      "Step 46 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 9, 10], 'root': 10, 'extensions': 'none'}\n",
      "Step 47 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 6, 10, 11], 'root': 11, 'extensions': 'none'}\n",
      "Step 48 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 10], 'root': 0, 'extensions': 'none'}\n",
      "Step 49 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 5, 8, 11], 'root': 1, 'extensions': 'none'}\n",
      "Step 50 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 6, 9], 'root': 2, 'extensions': 'none'}\n",
      "Step 51 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 7, 10], 'root': 3, 'extensions': 'none'}\n",
      "Step 52 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 4, 8, 11], 'root': 4, 'extensions': 'none'}\n",
      "Step 53 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 9], 'root': 5, 'extensions': 'none'}\n",
      "Step 54 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 10], 'root': 6, 'extensions': 'none'}\n",
      "Step 55 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 11], 'root': 7, 'extensions': 'none'}\n",
      "Step 56 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 8], 'root': 8, 'extensions': 'none'}\n",
      "Step 57 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 7, 9], 'root': 9, 'extensions': 'none'}\n",
      "Step 58 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 8, 10], 'root': 10, 'extensions': 'none'}\n",
      "Step 59 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [3, 6, 9, 11], 'root': 11, 'extensions': 'none'}\n",
      "Step 60 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 7, 10], 'root': 0, 'extensions': 'none'}\n",
      "Step 61 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 8, 11], 'root': 1, 'extensions': 'none'}\n",
      "Step 62 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 9], 'root': 2, 'extensions': 'none'}\n",
      "Step 63 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 6, 10], 'root': 3, 'extensions': 'none'}\n",
      "Step 64 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 7, 11], 'root': 4, 'extensions': 'none'}\n",
      "Step 65 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 8], 'root': 5, 'extensions': 'none'}\n",
      "Step 66 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 9], 'root': 6, 'extensions': 'none'}\n",
      "Step 67 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 10], 'root': 7, 'extensions': 'none'}\n",
      "Step 68 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [3, 6, 8, 11], 'root': 8, 'extensions': 'none'}\n",
      "Step 69 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 9], 'root': 9, 'extensions': 'none'}\n",
      "Step 70 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 5, 8, 10], 'root': 10, 'extensions': 'none'}\n",
      "Step 71 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 6, 9, 11], 'root': 11, 'extensions': 'none'}\n",
      "Step 72 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 1, 4, 7], 'root': 0, 'extensions': 1}\n",
      "Step 73 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 2, 5, 8], 'root': 1, 'extensions': 1}\n",
      "Step 74 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 3, 6, 9], 'root': 2, 'extensions': 1}\n",
      "Step 75 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 4, 7, 10], 'root': 3, 'extensions': 1}\n",
      "Step 76 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [4, 5, 8, 11], 'root': 4, 'extensions': 1}\n",
      "Step 77 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 5, 6, 9], 'root': 5, 'extensions': 1}\n",
      "Step 78 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 6, 7, 10], 'root': 6, 'extensions': 1}\n",
      "Step 79 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 7, 8, 11], 'root': 7, 'extensions': 1}\n",
      "Step 80 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 3, 8, 9], 'root': 8, 'extensions': 1}\n",
      "Step 81 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 4, 9, 10], 'root': 9, 'extensions': 1}\n",
      "Step 82 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 5, 10, 11], 'root': 10, 'extensions': 1}\n",
      "Step 83 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 3, 6, 11], 'root': 11, 'extensions': 1}\n",
      "Step 84 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 1, 3, 7], 'root': 0, 'extensions': 1}\n",
      "Step 85 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 2, 4, 8], 'root': 1, 'extensions': 1}\n",
      "Step 86 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 3, 5, 9], 'root': 2, 'extensions': 1}\n",
      "Step 87 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 4, 6, 10], 'root': 3, 'extensions': 1}\n",
      "Step 88 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [4, 5, 7, 11], 'root': 4, 'extensions': 1}\n",
      "Step 89 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 5, 6, 8], 'root': 5, 'extensions': 1}\n",
      "Step 90 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 6, 7, 9], 'root': 6, 'extensions': 1}\n",
      "Step 91 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 7, 8, 10], 'root': 7, 'extensions': 1}\n",
      "Step 92 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 8, 9, 11], 'root': 8, 'extensions': 1}\n",
      "Step 93 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 4, 9, 10], 'root': 9, 'extensions': 1}\n",
      "Step 94 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 5, 10, 11], 'root': 10, 'extensions': 1}\n",
      "Step 95 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 2, 6, 11], 'root': 11, 'extensions': 1}\n",
      "Step 96 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 7, 11], 'root': 0, 'extensions': 1}\n",
      "Step 97 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 2, 5, 8], 'root': 1, 'extensions': 1}\n",
      "Step 98 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 3, 6, 9], 'root': 2, 'extensions': 1}\n",
      "Step 99 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 4, 7, 10], 'root': 3, 'extensions': 1}\n",
      "Step 100 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 4, 5, 8, 11], 'root': 4, 'extensions': 1}\n",
      "Step 101 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 6, 9], 'root': 5, 'extensions': 1}\n",
      "Step 102 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 7, 10], 'root': 6, 'extensions': 1}\n",
      "Step 103 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 8, 11], 'root': 7, 'extensions': 1}\n",
      "Step 104 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 3, 7, 8, 9], 'root': 8, 'extensions': 1}\n",
      "Step 105 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 4, 8, 9, 10], 'root': 9, 'extensions': 1}\n",
      "Step 106 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 5, 9, 10, 11], 'root': 10, 'extensions': 1}\n",
      "Step 107 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 3, 6, 10, 11], 'root': 11, 'extensions': 1}\n",
      "Step 108 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 3, 7, 11], 'root': 0, 'extensions': 1}\n",
      "Step 109 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 2, 4, 8], 'root': 1, 'extensions': 1}\n",
      "Step 110 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 3, 5, 9], 'root': 2, 'extensions': 1}\n",
      "Step 111 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 4, 6, 10], 'root': 3, 'extensions': 1}\n",
      "Step 112 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 4, 5, 7, 11], 'root': 4, 'extensions': 1}\n",
      "Step 113 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 6, 8], 'root': 5, 'extensions': 1}\n",
      "Step 114 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 7, 9], 'root': 6, 'extensions': 1}\n",
      "Step 115 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 8, 10], 'root': 7, 'extensions': 1}\n",
      "Step 116 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 7, 8, 9, 11], 'root': 8, 'extensions': 1}\n",
      "Step 117 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 8, 9, 10], 'root': 9, 'extensions': 1}\n",
      "Step 118 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 9, 10, 11], 'root': 10, 'extensions': 1}\n",
      "Step 119 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 2, 6, 10, 11], 'root': 11, 'extensions': 1}\n",
      "Step 120 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 1, 4, 7, 10], 'root': 0, 'extensions': 1}\n",
      "Step 121 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 2, 5, 8, 11], 'root': 1, 'extensions': 1}\n",
      "Step 122 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 3, 6, 9], 'root': 2, 'extensions': 1}\n",
      "Step 123 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 4, 7, 10], 'root': 3, 'extensions': 1}\n",
      "Step 124 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 4, 5, 8, 11], 'root': 4, 'extensions': 1}\n",
      "Step 125 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 6, 9], 'root': 5, 'extensions': 1}\n",
      "Step 126 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 7, 10], 'root': 6, 'extensions': 1}\n",
      "Step 127 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 8, 11], 'root': 7, 'extensions': 1}\n",
      "Step 128 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 8, 9], 'root': 8, 'extensions': 1}\n",
      "Step 129 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 7, 9, 10], 'root': 9, 'extensions': 1}\n",
      "Step 130 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 8, 10, 11], 'root': 10, 'extensions': 1}\n",
      "Step 131 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 9, 11], 'root': 11, 'extensions': 1}\n",
      "Step 132 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 3, 7, 10], 'root': 0, 'extensions': 1}\n",
      "Step 133 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 2, 4, 8, 11], 'root': 1, 'extensions': 1}\n",
      "Step 134 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 3, 5, 9], 'root': 2, 'extensions': 1}\n",
      "Step 135 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 4, 6, 10], 'root': 3, 'extensions': 1}\n",
      "Step 136 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 5, 7, 11], 'root': 4, 'extensions': 1}\n",
      "Step 137 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 6, 8], 'root': 5, 'extensions': 1}\n",
      "Step 138 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 7, 9], 'root': 6, 'extensions': 1}\n",
      "Step 139 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 8, 10], 'root': 7, 'extensions': 1}\n",
      "Step 140 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [3, 6, 8, 9, 11], 'root': 8, 'extensions': 1}\n",
      "Step 141 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 9, 10], 'root': 9, 'extensions': 1}\n",
      "Step 142 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 5, 8, 10, 11], 'root': 10, 'extensions': 1}\n",
      "Step 143 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 6, 9, 11], 'root': 11, 'extensions': 1}\n",
      "Step 144 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 2, 4, 7], 'root': 0, 'extensions': 2}\n",
      "Step 145 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 3, 5, 8], 'root': 1, 'extensions': 2}\n",
      "Step 146 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 4, 6, 9], 'root': 2, 'extensions': 2}\n",
      "Step 147 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 5, 7, 10], 'root': 3, 'extensions': 2}\n",
      "Step 148 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [4, 6, 8, 11], 'root': 4, 'extensions': 2}\n",
      "Step 149 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 5, 7, 9], 'root': 5, 'extensions': 2}\n",
      "Step 150 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 6, 8, 10], 'root': 6, 'extensions': 2}\n",
      "Step 151 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 7, 9, 11], 'root': 7, 'extensions': 2}\n",
      "Step 152 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 3, 8, 10], 'root': 8, 'extensions': 2}\n",
      "Step 153 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 4, 9, 11], 'root': 9, 'extensions': 2}\n",
      "Step 154 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 2, 5, 10], 'root': 10, 'extensions': 2}\n",
      "Step 155 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 3, 6, 11], 'root': 11, 'extensions': 2}\n",
      "Step 156 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 2, 3, 7], 'root': 0, 'extensions': 2}\n",
      "Step 157 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 3, 4, 8], 'root': 1, 'extensions': 2}\n",
      "Step 158 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 4, 5, 9], 'root': 2, 'extensions': 2}\n",
      "Step 159 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 5, 6, 10], 'root': 3, 'extensions': 2}\n",
      "Step 160 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [4, 6, 7, 11], 'root': 4, 'extensions': 2}\n",
      "Step 161 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 5, 7, 8], 'root': 5, 'extensions': 2}\n",
      "Step 162 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 6, 8, 9], 'root': 6, 'extensions': 2}\n",
      "Step 163 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 7, 9, 10], 'root': 7, 'extensions': 2}\n",
      "Step 164 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 8, 10, 11], 'root': 8, 'extensions': 2}\n",
      "Step 165 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 4, 9, 11], 'root': 9, 'extensions': 2}\n",
      "Step 166 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 1, 5, 10], 'root': 10, 'extensions': 2}\n",
      "Step 167 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 2, 6, 11], 'root': 11, 'extensions': 2}\n",
      "Step 168 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 2, 4, 7, 11], 'root': 0, 'extensions': 2}\n",
      "Step 169 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 3, 5, 8], 'root': 1, 'extensions': 2}\n",
      "Step 170 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 4, 6, 9], 'root': 2, 'extensions': 2}\n",
      "Step 171 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 5, 7, 10], 'root': 3, 'extensions': 2}\n",
      "Step 172 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 4, 6, 8, 11], 'root': 4, 'extensions': 2}\n",
      "Step 173 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 7, 9], 'root': 5, 'extensions': 2}\n",
      "Step 174 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 8, 10], 'root': 6, 'extensions': 2}\n",
      "Step 175 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 9, 11], 'root': 7, 'extensions': 2}\n",
      "Step 176 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 3, 7, 8, 10], 'root': 8, 'extensions': 2}\n",
      "Step 177 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 4, 8, 9, 11], 'root': 9, 'extensions': 2}\n",
      "Step 178 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 2, 5, 9, 10], 'root': 10, 'extensions': 2}\n",
      "Step 179 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 3, 6, 10, 11], 'root': 11, 'extensions': 2}\n",
      "Step 180 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 2, 3, 7, 11], 'root': 0, 'extensions': 2}\n",
      "Step 181 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 3, 4, 8], 'root': 1, 'extensions': 2}\n",
      "Step 182 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 4, 5, 9], 'root': 2, 'extensions': 2}\n",
      "Step 183 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 5, 6, 10], 'root': 3, 'extensions': 2}\n",
      "Step 184 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 4, 6, 7, 11], 'root': 4, 'extensions': 2}\n",
      "Step 185 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 7, 8], 'root': 5, 'extensions': 2}\n",
      "Step 186 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 8, 9], 'root': 6, 'extensions': 2}\n",
      "Step 187 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 9, 10], 'root': 7, 'extensions': 2}\n",
      "Step 188 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 7, 8, 10, 11], 'root': 8, 'extensions': 2}\n",
      "Step 189 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 8, 9, 11], 'root': 9, 'extensions': 2}\n",
      "Step 190 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 9, 10], 'root': 10, 'extensions': 2}\n",
      "Step 191 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 10, 11], 'root': 11, 'extensions': 2}\n",
      "Step 192 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 4, 7, 10], 'root': 0, 'extensions': 2}\n",
      "Step 193 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 5, 8, 11], 'root': 1, 'extensions': 2}\n",
      "Step 194 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 4, 6, 9], 'root': 2, 'extensions': 2}\n",
      "Step 195 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 5, 7, 10], 'root': 3, 'extensions': 2}\n",
      "Step 196 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 4, 6, 8, 11], 'root': 4, 'extensions': 2}\n",
      "Step 197 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 7, 9], 'root': 5, 'extensions': 2}\n",
      "Step 198 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 8, 10], 'root': 6, 'extensions': 2}\n",
      "Step 199 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 9, 11], 'root': 7, 'extensions': 2}\n",
      "Step 200 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 8, 10], 'root': 8, 'extensions': 2}\n",
      "Step 201 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 7, 9, 11], 'root': 9, 'extensions': 2}\n",
      "Step 202 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 8, 10], 'root': 10, 'extensions': 2}\n",
      "Step 203 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 6, 9, 11], 'root': 11, 'extensions': 2}\n",
      "Step 204 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 3, 7, 10], 'root': 0, 'extensions': 2}\n",
      "Step 205 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 4, 8, 11], 'root': 1, 'extensions': 2}\n",
      "Step 206 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 4, 5, 9], 'root': 2, 'extensions': 2}\n",
      "Step 207 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 5, 6, 10], 'root': 3, 'extensions': 2}\n",
      "Step 208 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 6, 7, 11], 'root': 4, 'extensions': 2}\n",
      "Step 209 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 7, 8], 'root': 5, 'extensions': 2}\n",
      "Step 210 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 8, 9], 'root': 6, 'extensions': 2}\n",
      "Step 211 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 9, 10], 'root': 7, 'extensions': 2}\n",
      "Step 212 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [3, 6, 8, 10, 11], 'root': 8, 'extensions': 2}\n",
      "Step 213 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 9, 11], 'root': 9, 'extensions': 2}\n",
      "Step 214 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 5, 8, 10], 'root': 10, 'extensions': 2}\n",
      "Step 215 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 2, 6, 9, 11], 'root': 11, 'extensions': 2}\n",
      "Step 216 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 4, 7], 'root': 0, 'extensions': 4}\n",
      "Step 217 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 5, 8], 'root': 1, 'extensions': 4}\n",
      "Step 218 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 5, 6, 9], 'root': 2, 'extensions': 4}\n",
      "Step 219 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 6, 7, 10], 'root': 3, 'extensions': 4}\n",
      "Step 220 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [4, 7, 8, 11], 'root': 4, 'extensions': 4}\n",
      "Step 221 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 5, 8, 9], 'root': 5, 'extensions': 4}\n",
      "Step 222 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 6, 9, 10], 'root': 6, 'extensions': 4}\n",
      "Step 223 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 7, 10, 11], 'root': 7, 'extensions': 4}\n",
      "Step 224 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 8, 11], 'root': 8, 'extensions': 4}\n",
      "Step 225 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 1, 4, 9], 'root': 9, 'extensions': 4}\n",
      "Step 226 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 2, 5, 10], 'root': 10, 'extensions': 4}\n",
      "Step 227 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 3, 6, 11], 'root': 11, 'extensions': 4}\n",
      "Step 228 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 4, 7, 11], 'root': 0, 'extensions': 4}\n",
      "Step 229 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 5, 8], 'root': 1, 'extensions': 4}\n",
      "Step 230 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 6, 9], 'root': 2, 'extensions': 4}\n",
      "Step 231 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 6, 7, 10], 'root': 3, 'extensions': 4}\n",
      "Step 232 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 4, 7, 8, 11], 'root': 4, 'extensions': 4}\n",
      "Step 233 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 8, 9], 'root': 5, 'extensions': 4}\n",
      "Step 234 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 9, 10], 'root': 6, 'extensions': 4}\n",
      "Step 235 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 6, 7, 10, 11], 'root': 7, 'extensions': 4}\n",
      "Step 236 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 7, 8, 11], 'root': 8, 'extensions': 4}\n",
      "Step 237 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 8, 9], 'root': 9, 'extensions': 4}\n",
      "Step 238 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 9, 10], 'root': 10, 'extensions': 4}\n",
      "Step 239 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 6, 10, 11], 'root': 11, 'extensions': 4}\n",
      "Step 240 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 4, 7, 10], 'root': 0, 'extensions': 4}\n",
      "Step 241 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 5, 8, 11], 'root': 1, 'extensions': 4}\n",
      "Step 242 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 6, 9], 'root': 2, 'extensions': 4}\n",
      "Step 243 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 6, 7, 10], 'root': 3, 'extensions': 4}\n",
      "Step 244 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 7, 8, 11], 'root': 4, 'extensions': 4}\n",
      "Step 245 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 8, 9], 'root': 5, 'extensions': 4}\n",
      "Step 246 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 9, 10], 'root': 6, 'extensions': 4}\n",
      "Step 247 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 5, 7, 10, 11], 'root': 7, 'extensions': 4}\n",
      "Step 248 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 8, 11], 'root': 8, 'extensions': 4}\n",
      "Step 249 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 4, 7, 9], 'root': 9, 'extensions': 4}\n",
      "Step 250 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 2, 5, 8, 10], 'root': 10, 'extensions': 4}\n",
      "Step 251 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 3, 6, 9, 11], 'root': 11, 'extensions': 4}\n",
      "Step 252 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 4, 5, 7], 'root': 0, 'extensions': 5}\n",
      "Step 253 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 5, 6, 8], 'root': 1, 'extensions': 5}\n",
      "Step 254 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 6, 7, 9], 'root': 2, 'extensions': 5}\n",
      "Step 255 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 7, 8, 10], 'root': 3, 'extensions': 5}\n",
      "Step 256 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [4, 8, 9, 11], 'root': 4, 'extensions': 5}\n",
      "Step 257 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 5, 9, 10], 'root': 5, 'extensions': 5}\n",
      "Step 258 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 6, 10, 11], 'root': 6, 'extensions': 5}\n",
      "Step 259 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 2, 7, 11], 'root': 7, 'extensions': 5}\n",
      "Step 260 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 1, 3, 8], 'root': 8, 'extensions': 5}\n",
      "Step 261 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 2, 4, 9], 'root': 9, 'extensions': 5}\n",
      "Step 262 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 3, 5, 10], 'root': 10, 'extensions': 5}\n",
      "Step 263 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 4, 6, 11], 'root': 11, 'extensions': 5}\n",
      "Step 264 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 5, 7], 'root': 0, 'extensions': 5}\n",
      "Step 265 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 6, 8], 'root': 1, 'extensions': 5}\n",
      "Step 266 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 5, 7, 9], 'root': 2, 'extensions': 5}\n",
      "Step 267 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 6, 8, 10], 'root': 3, 'extensions': 5}\n",
      "Step 268 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [4, 7, 9, 11], 'root': 4, 'extensions': 5}\n",
      "Step 269 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 5, 8, 10], 'root': 5, 'extensions': 5}\n",
      "Step 270 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 6, 9, 11], 'root': 6, 'extensions': 5}\n",
      "Step 271 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 2, 7, 10], 'root': 7, 'extensions': 5}\n",
      "Step 272 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 3, 8, 11], 'root': 8, 'extensions': 5}\n",
      "Step 273 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 2, 4, 9], 'root': 9, 'extensions': 5}\n",
      "Step 274 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 3, 5, 10], 'root': 10, 'extensions': 5}\n",
      "Step 275 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 4, 6, 11], 'root': 11, 'extensions': 5}\n",
      "Step 276 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 7, 11], 'root': 0, 'extensions': 5}\n",
      "Step 277 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 6, 8], 'root': 1, 'extensions': 5}\n",
      "Step 278 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 7, 9], 'root': 2, 'extensions': 5}\n",
      "Step 279 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 7, 8, 10], 'root': 3, 'extensions': 5}\n",
      "Step 280 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 4, 8, 9, 11], 'root': 4, 'extensions': 5}\n",
      "Step 281 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 9, 10], 'root': 5, 'extensions': 5}\n",
      "Step 282 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 10, 11], 'root': 6, 'extensions': 5}\n",
      "Step 283 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 2, 6, 7, 11], 'root': 7, 'extensions': 5}\n",
      "Step 284 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 3, 7, 8], 'root': 8, 'extensions': 5}\n",
      "Step 285 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 4, 8, 9], 'root': 9, 'extensions': 5}\n",
      "Step 286 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 5, 9, 10], 'root': 10, 'extensions': 5}\n",
      "Step 287 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 4, 6, 10, 11], 'root': 11, 'extensions': 5}\n",
      "Step 288 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 5, 7, 11], 'root': 0, 'extensions': 5}\n",
      "Step 289 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 6, 8], 'root': 1, 'extensions': 5}\n",
      "Step 290 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 7, 9], 'root': 2, 'extensions': 5}\n",
      "Step 291 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 6, 8, 10], 'root': 3, 'extensions': 5}\n",
      "Step 292 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 4, 7, 9, 11], 'root': 4, 'extensions': 5}\n",
      "Step 293 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 8, 10], 'root': 5, 'extensions': 5}\n",
      "Step 294 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 6, 9, 11], 'root': 6, 'extensions': 5}\n",
      "Step 295 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 2, 6, 7, 10], 'root': 7, 'extensions': 5}\n",
      "Step 296 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 3, 7, 8, 11], 'root': 8, 'extensions': 5}\n",
      "Step 297 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 2, 4, 8, 9], 'root': 9, 'extensions': 5}\n",
      "Step 298 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 3, 5, 9, 10], 'root': 10, 'extensions': 5}\n",
      "Step 299 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 4, 6, 10, 11], 'root': 11, 'extensions': 5}\n",
      "Step 300 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 4, 5, 7, 10], 'root': 0, 'extensions': 5}\n",
      "Step 301 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 5, 6, 8, 11], 'root': 1, 'extensions': 5}\n",
      "Step 302 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 6, 7, 9], 'root': 2, 'extensions': 5}\n",
      "Step 303 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 7, 8, 10], 'root': 3, 'extensions': 5}\n",
      "Step 304 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 4, 8, 9, 11], 'root': 4, 'extensions': 5}\n",
      "Step 305 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 9, 10], 'root': 5, 'extensions': 5}\n",
      "Step 306 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 10, 11], 'root': 6, 'extensions': 5}\n",
      "Step 307 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 7, 11], 'root': 7, 'extensions': 5}\n",
      "Step 308 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 1, 3, 6, 8], 'root': 8, 'extensions': 5}\n",
      "Step 309 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 2, 4, 7, 9], 'root': 9, 'extensions': 5}\n",
      "Step 310 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 3, 5, 8, 10], 'root': 10, 'extensions': 5}\n",
      "Step 311 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [3, 4, 6, 9, 11], 'root': 11, 'extensions': 5}\n",
      "Step 312 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 7, 10], 'root': 0, 'extensions': 5}\n",
      "Step 313 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 8, 11], 'root': 1, 'extensions': 5}\n",
      "Step 314 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 7, 9], 'root': 2, 'extensions': 5}\n",
      "Step 315 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 6, 8, 10], 'root': 3, 'extensions': 5}\n",
      "Step 316 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 7, 9, 11], 'root': 4, 'extensions': 5}\n",
      "Step 317 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 8, 10], 'root': 5, 'extensions': 5}\n",
      "Step 318 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 6, 9, 11], 'root': 6, 'extensions': 5}\n",
      "Step 319 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 7, 10], 'root': 7, 'extensions': 5}\n",
      "Step 320 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 6, 8, 11], 'root': 8, 'extensions': 5}\n",
      "Step 321 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 4, 7, 9], 'root': 9, 'extensions': 5}\n",
      "Step 322 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 5, 8, 10], 'root': 10, 'extensions': 5}\n",
      "Step 323 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 6, 9, 11], 'root': 11, 'extensions': 5}\n",
      "Step 324 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 4, 6, 7], 'root': 0, 'extensions': 6}\n",
      "Step 325 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 5, 7, 8], 'root': 1, 'extensions': 6}\n",
      "Step 326 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 6, 8, 9], 'root': 2, 'extensions': 6}\n",
      "Step 327 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 7, 9, 10], 'root': 3, 'extensions': 6}\n",
      "Step 328 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [4, 8, 10, 11], 'root': 4, 'extensions': 6}\n",
      "Step 329 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 5, 9, 11], 'root': 5, 'extensions': 6}\n",
      "Step 330 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 1, 6, 10], 'root': 6, 'extensions': 6}\n",
      "Step 331 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 2, 7, 11], 'root': 7, 'extensions': 6}\n",
      "Step 332 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 2, 3, 8], 'root': 8, 'extensions': 6}\n",
      "Step 333 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 3, 4, 9], 'root': 9, 'extensions': 6}\n",
      "Step 334 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 4, 5, 10], 'root': 10, 'extensions': 6}\n",
      "Step 335 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 5, 6, 11], 'root': 11, 'extensions': 6}\n",
      "Step 336 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 6, 7], 'root': 0, 'extensions': 6}\n",
      "Step 337 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 7, 8], 'root': 1, 'extensions': 6}\n",
      "Step 338 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 5, 8, 9], 'root': 2, 'extensions': 6}\n",
      "Step 339 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 6, 9, 10], 'root': 3, 'extensions': 6}\n",
      "Step 340 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [4, 7, 10, 11], 'root': 4, 'extensions': 6}\n",
      "Step 341 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 5, 8, 11], 'root': 5, 'extensions': 6}\n",
      "Step 342 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 1, 6, 9], 'root': 6, 'extensions': 6}\n",
      "Step 343 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 2, 7, 10], 'root': 7, 'extensions': 6}\n",
      "Step 344 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 3, 8, 11], 'root': 8, 'extensions': 6}\n",
      "Step 345 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 4, 9], 'root': 9, 'extensions': 6}\n",
      "Step 346 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 5, 10], 'root': 10, 'extensions': 6}\n",
      "Step 347 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 5, 6, 11], 'root': 11, 'extensions': 6}\n",
      "Step 348 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 6, 7, 11], 'root': 0, 'extensions': 6}\n",
      "Step 349 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 7, 8], 'root': 1, 'extensions': 6}\n",
      "Step 350 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 8, 9], 'root': 2, 'extensions': 6}\n",
      "Step 351 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 7, 9, 10], 'root': 3, 'extensions': 6}\n",
      "Step 352 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 4, 8, 10, 11], 'root': 4, 'extensions': 6}\n",
      "Step 353 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 9, 11], 'root': 5, 'extensions': 6}\n",
      "Step 354 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 6, 10], 'root': 6, 'extensions': 6}\n",
      "Step 355 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 7, 11], 'root': 7, 'extensions': 6}\n",
      "Step 356 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 2, 3, 7, 8], 'root': 8, 'extensions': 6}\n",
      "Step 357 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 3, 4, 8, 9], 'root': 9, 'extensions': 6}\n",
      "Step 358 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 4, 5, 9, 10], 'root': 10, 'extensions': 6}\n",
      "Step 359 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 5, 6, 10, 11], 'root': 11, 'extensions': 6}\n",
      "Step 360 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 6, 7, 11], 'root': 0, 'extensions': 6}\n",
      "Step 361 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 7, 8], 'root': 1, 'extensions': 6}\n",
      "Step 362 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 8, 9], 'root': 2, 'extensions': 6}\n",
      "Step 363 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 6, 9, 10], 'root': 3, 'extensions': 6}\n",
      "Step 364 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 4, 7, 10, 11], 'root': 4, 'extensions': 6}\n",
      "Step 365 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 5, 8, 11], 'root': 5, 'extensions': 6}\n",
      "Step 366 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 6, 9], 'root': 6, 'extensions': 6}\n",
      "Step 367 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 7, 10], 'root': 7, 'extensions': 6}\n",
      "Step 368 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 3, 7, 8, 11], 'root': 8, 'extensions': 6}\n",
      "Step 369 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 4, 8, 9], 'root': 9, 'extensions': 6}\n",
      "Step 370 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 4, 5, 9, 10], 'root': 10, 'extensions': 6}\n",
      "Step 371 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 5, 6, 10, 11], 'root': 11, 'extensions': 6}\n",
      "Step 372 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 4, 6, 7, 10], 'root': 0, 'extensions': 6}\n",
      "Step 373 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 5, 7, 8, 11], 'root': 1, 'extensions': 6}\n",
      "Step 374 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 6, 8, 9], 'root': 2, 'extensions': 6}\n",
      "Step 375 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 7, 9, 10], 'root': 3, 'extensions': 6}\n",
      "Step 376 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 4, 8, 10, 11], 'root': 4, 'extensions': 6}\n",
      "Step 377 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 9, 11], 'root': 5, 'extensions': 6}\n",
      "Step 378 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 1, 4, 6, 10], 'root': 6, 'extensions': 6}\n",
      "Step 379 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 2, 5, 7, 11], 'root': 7, 'extensions': 6}\n",
      "Step 380 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 3, 6, 8], 'root': 8, 'extensions': 6}\n",
      "Step 381 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 4, 7, 9], 'root': 9, 'extensions': 6}\n",
      "Step 382 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 4, 5, 8, 10], 'root': 10, 'extensions': 6}\n",
      "Step 383 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [3, 5, 6, 9, 11], 'root': 11, 'extensions': 6}\n",
      "Step 384 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 7, 10], 'root': 0, 'extensions': 6}\n",
      "Step 385 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 7, 8, 11], 'root': 1, 'extensions': 6}\n",
      "Step 386 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 8, 9], 'root': 2, 'extensions': 6}\n",
      "Step 387 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 6, 9, 10], 'root': 3, 'extensions': 6}\n",
      "Step 388 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 7, 10, 11], 'root': 4, 'extensions': 6}\n",
      "Step 389 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 5, 8, 11], 'root': 5, 'extensions': 6}\n",
      "Step 390 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 4, 6, 9], 'root': 6, 'extensions': 6}\n",
      "Step 391 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 2, 5, 7, 10], 'root': 7, 'extensions': 6}\n",
      "Step 392 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 3, 6, 8, 11], 'root': 8, 'extensions': 6}\n",
      "Step 393 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 4, 7, 9], 'root': 9, 'extensions': 6}\n",
      "Step 394 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 5, 8, 10], 'root': 10, 'extensions': 6}\n",
      "Step 395 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 5, 6, 9, 11], 'root': 11, 'extensions': 6}\n",
      "Step 396 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 4, 7, 8], 'root': 0, 'extensions': 8}\n",
      "Step 397 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 5, 8, 9], 'root': 1, 'extensions': 8}\n",
      "Step 398 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 6, 9, 10], 'root': 2, 'extensions': 8}\n",
      "Step 399 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 7, 10, 11], 'root': 3, 'extensions': 8}\n",
      "Step 400 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 4, 8, 11], 'root': 4, 'extensions': 8}\n",
      "Step 401 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 1, 5, 9], 'root': 5, 'extensions': 8}\n",
      "Step 402 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 2, 6, 10], 'root': 6, 'extensions': 8}\n",
      "Step 403 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 3, 7, 11], 'root': 7, 'extensions': 8}\n",
      "Step 404 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [0, 3, 4, 8], 'root': 8, 'extensions': 8}\n",
      "Step 405 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [1, 4, 5, 9], 'root': 9, 'extensions': 8}\n",
      "Step 406 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [2, 5, 6, 10], 'root': 10, 'extensions': 8}\n",
      "Step 407 of 492: {'majmin3': 'maj', 'majmin7': 'none', 'pitch_class': [3, 6, 7, 11], 'root': 11, 'extensions': 8}\n",
      "Step 408 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 4, 7, 8, 11], 'root': 0, 'extensions': 8}\n",
      "Step 409 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 5, 8, 9], 'root': 1, 'extensions': 8}\n",
      "Step 410 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 6, 9, 10], 'root': 2, 'extensions': 8}\n",
      "Step 411 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 7, 10, 11], 'root': 3, 'extensions': 8}\n",
      "Step 412 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 3, 4, 8, 11], 'root': 4, 'extensions': 8}\n",
      "Step 413 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 5, 9], 'root': 5, 'extensions': 8}\n",
      "Step 414 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 6, 10], 'root': 6, 'extensions': 8}\n",
      "Step 415 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 3, 6, 7, 11], 'root': 7, 'extensions': 8}\n",
      "Step 416 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [0, 3, 4, 7, 8], 'root': 8, 'extensions': 8}\n",
      "Step 417 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [1, 4, 5, 8, 9], 'root': 9, 'extensions': 8}\n",
      "Step 418 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [2, 5, 6, 9, 10], 'root': 10, 'extensions': 8}\n",
      "Step 419 of 492: {'majmin3': 'maj', 'majmin7': 'maj', 'pitch_class': [3, 6, 7, 10, 11], 'root': 11, 'extensions': 8}\n",
      "Step 420 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 8, 10], 'root': 0, 'extensions': 8}\n",
      "Step 421 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 5, 8, 9, 11], 'root': 1, 'extensions': 8}\n",
      "Step 422 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 6, 9, 10], 'root': 2, 'extensions': 8}\n",
      "Step 423 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 3, 7, 10, 11], 'root': 3, 'extensions': 8}\n",
      "Step 424 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 2, 4, 8, 11], 'root': 4, 'extensions': 8}\n",
      "Step 425 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 1, 3, 5, 9], 'root': 5, 'extensions': 8}\n",
      "Step 426 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 2, 4, 6, 10], 'root': 6, 'extensions': 8}\n",
      "Step 427 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 3, 5, 7, 11], 'root': 7, 'extensions': 8}\n",
      "Step 428 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 4, 6, 8], 'root': 8, 'extensions': 8}\n",
      "Step 429 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 5, 7, 9], 'root': 9, 'extensions': 8}\n",
      "Step 430 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 6, 8, 10], 'root': 10, 'extensions': 8}\n",
      "Step 431 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [3, 6, 7, 9, 11], 'root': 11, 'extensions': 8}\n",
      "Step 432 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 7, 9], 'root': 0, 'extensions': 9}\n",
      "Step 433 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 8, 10], 'root': 1, 'extensions': 9}\n",
      "Step 434 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 5, 9, 11], 'root': 2, 'extensions': 9}\n",
      "Step 435 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 3, 6, 10], 'root': 3, 'extensions': 9}\n",
      "Step 436 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 4, 7, 11], 'root': 4, 'extensions': 9}\n",
      "Step 437 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 2, 5, 8], 'root': 5, 'extensions': 9}\n",
      "Step 438 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 3, 6, 9], 'root': 6, 'extensions': 9}\n",
      "Step 439 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 4, 7, 10], 'root': 7, 'extensions': 9}\n",
      "Step 440 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [3, 5, 8, 11], 'root': 8, 'extensions': 9}\n",
      "Step 441 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [0, 4, 6, 9], 'root': 9, 'extensions': 9}\n",
      "Step 442 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [1, 5, 7, 10], 'root': 10, 'extensions': 9}\n",
      "Step 443 of 492: {'majmin3': 'min', 'majmin7': 'none', 'pitch_class': [2, 6, 8, 11], 'root': 11, 'extensions': 9}\n",
      "Step 444 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 3, 7, 9, 11], 'root': 0, 'extensions': 9}\n",
      "Step 445 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 1, 4, 8, 10], 'root': 1, 'extensions': 9}\n",
      "Step 446 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 2, 5, 9, 11], 'root': 2, 'extensions': 9}\n",
      "Step 447 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 2, 3, 6, 10], 'root': 3, 'extensions': 9}\n",
      "Step 448 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 3, 4, 7, 11], 'root': 4, 'extensions': 9}\n",
      "Step 449 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 2, 4, 5, 8], 'root': 5, 'extensions': 9}\n",
      "Step 450 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 3, 5, 6, 9], 'root': 6, 'extensions': 9}\n",
      "Step 451 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 4, 6, 7, 10], 'root': 7, 'extensions': 9}\n",
      "Step 452 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [3, 5, 7, 8, 11], 'root': 8, 'extensions': 9}\n",
      "Step 453 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [0, 4, 6, 8, 9], 'root': 9, 'extensions': 9}\n",
      "Step 454 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [1, 5, 7, 9, 10], 'root': 10, 'extensions': 9}\n",
      "Step 455 of 492: {'majmin3': 'min', 'majmin7': 'maj', 'pitch_class': [2, 6, 8, 10, 11], 'root': 11, 'extensions': 9}\n",
      "Step 456 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 7, 9, 10], 'root': 0, 'extensions': 9}\n",
      "Step 457 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 8, 10, 11], 'root': 1, 'extensions': 9}\n",
      "Step 458 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 5, 9, 11], 'root': 2, 'extensions': 9}\n",
      "Step 459 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 3, 6, 10], 'root': 3, 'extensions': 9}\n",
      "Step 460 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 2, 4, 7, 11], 'root': 4, 'extensions': 9}\n",
      "Step 461 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 2, 3, 5, 8], 'root': 5, 'extensions': 9}\n",
      "Step 462 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 3, 4, 6, 9], 'root': 6, 'extensions': 9}\n",
      "Step 463 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 4, 5, 7, 10], 'root': 7, 'extensions': 9}\n",
      "Step 464 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [3, 5, 6, 8, 11], 'root': 8, 'extensions': 9}\n",
      "Step 465 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 4, 6, 7, 9], 'root': 9, 'extensions': 9}\n",
      "Step 466 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 5, 7, 8, 10], 'root': 10, 'extensions': 9}\n",
      "Step 467 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 6, 8, 9, 11], 'root': 11, 'extensions': 9}\n",
      "Step 468 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 10, 11], 'root': 0, 'extensions': 11}\n",
      "Step 469 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 1, 5, 8, 11], 'root': 1, 'extensions': 11}\n",
      "Step 470 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 1, 2, 6, 9], 'root': 2, 'extensions': 11}\n",
      "Step 471 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 2, 3, 7, 10], 'root': 3, 'extensions': 11}\n",
      "Step 472 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 3, 4, 8, 11], 'root': 4, 'extensions': 11}\n",
      "Step 473 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 4, 5, 9], 'root': 5, 'extensions': 11}\n",
      "Step 474 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 5, 6, 10], 'root': 6, 'extensions': 11}\n",
      "Step 475 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 6, 7, 11], 'root': 7, 'extensions': 11}\n",
      "Step 476 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [0, 3, 6, 7, 8], 'root': 8, 'extensions': 11}\n",
      "Step 477 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [1, 4, 7, 8, 9], 'root': 9, 'extensions': 11}\n",
      "Step 478 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [2, 5, 8, 9, 10], 'root': 10, 'extensions': 11}\n",
      "Step 479 of 492: {'majmin3': 'maj', 'majmin7': 'min', 'pitch_class': [3, 6, 9, 10, 11], 'root': 11, 'extensions': 11}\n",
      "Step 480 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 7, 10, 11], 'root': 0, 'extensions': 11}\n",
      "Step 481 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 4, 8, 11], 'root': 1, 'extensions': 11}\n",
      "Step 482 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 1, 2, 5, 9], 'root': 2, 'extensions': 11}\n",
      "Step 483 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 2, 3, 6, 10], 'root': 3, 'extensions': 11}\n",
      "Step 484 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 3, 4, 7, 11], 'root': 4, 'extensions': 11}\n",
      "Step 485 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 3, 4, 5, 8], 'root': 5, 'extensions': 11}\n",
      "Step 486 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 4, 5, 6, 9], 'root': 6, 'extensions': 11}\n",
      "Step 487 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 5, 6, 7, 10], 'root': 7, 'extensions': 11}\n",
      "Step 488 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [3, 6, 7, 8, 11], 'root': 8, 'extensions': 11}\n",
      "Step 489 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [0, 4, 7, 8, 9], 'root': 9, 'extensions': 11}\n",
      "Step 490 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [1, 5, 8, 9, 10], 'root': 10, 'extensions': 11}\n",
      "Step 491 of 492: {'majmin3': 'min', 'majmin7': 'min', 'pitch_class': [2, 6, 9, 10, 11], 'root': 11, 'extensions': 11}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tmp3(j):\n",
    "    n_steps = 0\n",
    "    chord_fft = make_chord(label_to_chord[i]['root'],samples,majmin3=label_to_chord[i]['majmin3'],majmin7=label_to_chord[i]['majmin7'])\n",
    "    #chord_fft = make_chord_with_pitch_shift(label_to_chord[i]['root'],samples,sr,majmin3=label_to_chord[i]['majmin3'],majmin7=label_to_chord[i]['majmin7'],n_steps=n_steps)\n",
    "    tmp_dict = {'fft':[chord_fft],'target':i,'pitch_shift':n_steps}\n",
    "    tmp_dict.update(label_to_chord[i])\n",
    "    tmp_dict.update({'repetition':j})\n",
    "    return tmp_dict\n",
    "\n",
    "\n",
    "            \n",
    "label_to_chord = []\n",
    "n = -1\n",
    "set_created = False\n",
    "for l in ('none', 1, 2, 3, 4, 5, 6, 8, 9, 10, 11):  #skip 7\n",
    "    for k in ('none','maj','min'):\n",
    "        for j in ('maj','min'):\n",
    "            for i in range(12):\n",
    "                if j=='maj' and l==4: continue\n",
    "                if j=='maj' and l==3: continue #so we don't majmin3==maj+3 as well as majmin3==min+14\n",
    "                if j=='min' and l==3: continue #leaves only majmin3==min+4\n",
    "                if k=='none' and l==11: continue\n",
    "                if k=='none' and l==10: continue\n",
    "                if k=='maj' and l==11: continue\n",
    "                if k=='maj' and l==10: continue #so we don't majmin7==maj+10 as well as majmin7==min+11\n",
    "                if k=='min' and l==10: continue #leaves only majmin7==min+11\n",
    "                all_notes = set([0,7])\n",
    "                if j=='maj': all_notes.add(4)\n",
    "                if j=='min': all_notes.add(3)\n",
    "                if k=='maj': all_notes.add(11)\n",
    "                if k=='min': all_notes.add(10)\n",
    "                if l!='none': all_notes.add(l)\n",
    "                all_notes = frozenset([(x+i)%12 for x in all_notes])\n",
    "                if set_created and all_notes in set_of_all_chords: \n",
    "                    #print(all_notes)\n",
    "                    continue\n",
    "                if not set_created: \n",
    "                    set_of_all_chords = {all_notes}\n",
    "                    set_created = True\n",
    "                else: \n",
    "                    set_of_all_chords.add(all_notes)\n",
    "                #k = 'none'\n",
    "                n += 1\n",
    "                label_to_chord.append({'root':i, 'majmin3': j, 'majmin7': k, 'extensions': l, 'pitch_class': sorted(list(all_notes))}) #it's annoying that list and ds behavior is different here\n",
    "        \n",
    "\n",
    "for i in range(len(label_to_chord)):\n",
    "    print(\"Step %g of %g: \"%(i,len(label_to_chord)) + str(label_to_chord[i]))\n",
    "    if [label_to_chord[i]['extensions']=='none']: num_reps_tmp = num_reps*2\n",
    "    else: num_reps_tmp = num_reps\n",
    "    parallel_output = Parallel(n_jobs=num_cores)(delayed(tmp3)(i) for i in range(num_reps_tmp))\n",
    "    ds = ds.append(parallel_output,ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extensions</th>\n",
       "      <th>fft</th>\n",
       "      <th>majmin3</th>\n",
       "      <th>majmin7</th>\n",
       "      <th>pitch_class</th>\n",
       "      <th>pitch_shift</th>\n",
       "      <th>repetition</th>\n",
       "      <th>root</th>\n",
       "      <th>target</th>\n",
       "      <th>multi_hot_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42875</th>\n",
       "      <td>11</td>\n",
       "      <td>[[0.000317645216402, 0.000330283532693, 0.0003...</td>\n",
       "      <td>min</td>\n",
       "      <td>min</td>\n",
       "      <td>[2, 6, 9, 10, 11]</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>11.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42876</th>\n",
       "      <td>11</td>\n",
       "      <td>[[0.000232915756681, 0.000282462013064, 0.0003...</td>\n",
       "      <td>min</td>\n",
       "      <td>min</td>\n",
       "      <td>[2, 6, 9, 10, 11]</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>11.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42877</th>\n",
       "      <td>11</td>\n",
       "      <td>[[0.000317274116683, 0.000331640555004, 0.0003...</td>\n",
       "      <td>min</td>\n",
       "      <td>min</td>\n",
       "      <td>[2, 6, 9, 10, 11]</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>11.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42878</th>\n",
       "      <td>11</td>\n",
       "      <td>[[0.000264476459399, 0.00030275802486, 0.00031...</td>\n",
       "      <td>min</td>\n",
       "      <td>min</td>\n",
       "      <td>[2, 6, 9, 10, 11]</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>11.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42879</th>\n",
       "      <td>11</td>\n",
       "      <td>[[0.000280290193016, 0.000305630328844, 0.0003...</td>\n",
       "      <td>min</td>\n",
       "      <td>min</td>\n",
       "      <td>[2, 6, 9, 10, 11]</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>11.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      extensions                                                fft majmin3  \\\n",
       "42875         11  [[0.000317645216402, 0.000330283532693, 0.0003...     min   \n",
       "42876         11  [[0.000232915756681, 0.000282462013064, 0.0003...     min   \n",
       "42877         11  [[0.000317274116683, 0.000331640555004, 0.0003...     min   \n",
       "42878         11  [[0.000264476459399, 0.00030275802486, 0.00031...     min   \n",
       "42879         11  [[0.000280290193016, 0.000305630328844, 0.0003...     min   \n",
       "\n",
       "      majmin7        pitch_class  pitch_shift  repetition  root  target  \\\n",
       "42875     min  [2, 6, 9, 10, 11]            0          59  11.0   491.0   \n",
       "42876     min  [2, 6, 9, 10, 11]            0          60  11.0   491.0   \n",
       "42877     min  [2, 6, 9, 10, 11]            0          61  11.0   491.0   \n",
       "42878     min  [2, 6, 9, 10, 11]            0          62  11.0   491.0   \n",
       "42879     min  [2, 6, 9, 10, 11]            0          63  11.0   491.0   \n",
       "\n",
       "                                      multi_hot_encoding  \n",
       "42875  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "42876  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "42877  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "42878  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
       "42879  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ds[\"one_hot_encoding\"] = [to1hot(x,len(label_to_chord)) for x in ds['target']]\n",
    "ds[\"multi_hot_encoding\"] = [toMultiHot(x,12) for x in ds['pitch_class']]\n",
    "ds.head()\n",
    "ds.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(index=range(ds.shape[0]))\n",
    "tmp['harmony_pitch_class'] = None\n",
    "for i in range(ds.shape[0]):\n",
    "    num_pitches = np.sum(ds['multi_hot_encoding'][i])\n",
    "    if num_pitches >=3:\n",
    "        curr_pitches = [int((ds['root'][i])%12)]\n",
    "        curr_pitches += [int((ds['root'][i]+7)%12)]\n",
    "        if ds['majmin3'][i]=='maj':\n",
    "            curr_pitches += [int((ds['root'][i]+4)%12)]\n",
    "        else:\n",
    "            curr_pitches += [int((ds['root'][i]+3)%12)]\n",
    "        if ds['majmin7'][i]=='maj':\n",
    "            curr_pitches += [int((ds['root'][i]+11)%12)]\n",
    "        elif ds['majmin7'][i]=='min':\n",
    "            curr_pitches += [int((ds['root'][i]+10)%12)]\n",
    "        tmp['harmony_pitch_class'][i] = curr_pitches\n",
    "    else:\n",
    "        tmp['harmony_pitch_class'][i] = ds['pitch_class'][i]\n",
    "\n",
    "tmp['multi_hot_harmony'] = [toMultiHot(x,12) for x in tmp['harmony_pitch_class']]\n",
    "ds['harmony_pitch_class'] = tmp['harmony_pitch_class']\n",
    "ds['multi_hot_harmony'] = tmp['multi_hot_harmony']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(42880, 12)\n",
      "(42240, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33792,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_ind = np.where([sum(x)>0 for x in ds['multi_hot_encoding']])\n",
    "#good_ind = np.where([x=='none' for x in ds['extensions']])\n",
    "print(len(good_ind))\n",
    "ds_sub = ds.iloc[good_ind]\n",
    "\n",
    "#ds_sub = ds[ds['extensions']=='none']\n",
    "\n",
    "print(ds.shape)\n",
    "print(ds_sub.shape)\n",
    "\n",
    "num_ind = ds_sub.shape[0]\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 1 - train_ratio - validation_ratio\n",
    "train_ind = np.random.choice(num_ind,int(np.floor(num_ind*train_ratio)),replace=False)\n",
    "rest_ind = np.setdiff1d(range(num_ind),train_ind)\n",
    "validation_ind = np.random.choice(rest_ind,int(np.floor(len(rest_ind)*(validation_ratio/(validation_ratio+test_ratio)))),replace=False)\n",
    "test_ind = np.setdiff1d(rest_ind,validation_ind)\n",
    "\n",
    "train_data = ds_sub.iloc[train_ind]\n",
    "validation_data = ds_sub.iloc[validation_ind]\n",
    "test_data = ds_sub.iloc[test_ind]\n",
    "\n",
    "train_data.fft.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Gotta adjust these shape parameters\n",
    "train_x = np.vstack(train_data['fft']).reshape(train_data.shape[0],len(train_data['fft'].iloc[0][0]),1,1).astype(np.float32)\n",
    "train_y = np.vstack(train_data[\"multi_hot_harmony\"])\n",
    "train_size = train_y.shape[0]\n",
    "validation_x = np.vstack(validation_data['fft']).reshape(validation_data.shape[0],len(validation_data['fft'].iloc[0][0]),1,1).astype(np.float32)\n",
    "validation_y = np.vstack(validation_data[\"multi_hot_harmony\"])\n",
    "test_x = np.vstack(test_data['fft']).reshape(test_data.shape[0],len(validation_data['fft'].iloc[0][0]),1,1).astype(np.float32)\n",
    "test_y = np.vstack(test_data[\"multi_hot_harmony\"])\n",
    "\n",
    "#shuffle the training_data\n",
    "rand_ind = np.random.choice(train_x.shape[0],train_x.shape[0],replace=False)\n",
    "train_x = train_x[rand_ind,:]\n",
    "train_y = train_y[rand_ind,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33792, 1025, 1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = min(train_x.shape[0],3000) # we have so little data, just set the batch size to the entire training set\n",
    "NUM_CHANNELS = 1 \n",
    "NUM_LABELS = train_y.shape[1]\n",
    "STEP_UPDATE = 100\n",
    "\n",
    "SEED = 27\n",
    "FC1_SIZE = 512\n",
    "FC2_SIZE = 256\n",
    "FC3_SIZE = 128\n",
    "FC4_SIZE = 64\n",
    "ADAM_SIZE = 5e-5 #5e-5\n",
    "REGULARIZER_SIZE = 1e-5#1e-4\n",
    "\n",
    "MAX_STEPS = 100000\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, train_x.shape[1],train_x.shape[2],train_x.shape[3]],name=\"x-in\")\n",
    "true_y = tf.placeholder(tf.float32, [None, NUM_LABELS],name=\"y-in\")\n",
    "keep_prob1 = tf.placeholder(\"float\")\n",
    "keep_prob2 = tf.placeholder(\"float\")\n",
    "keep_prob3 = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "\n",
    "input_length = train_x.shape[1]*train_x.shape[2]*train_x.shape[3]\n",
    "input_flat = tf.reshape(inputs,[-1, input_length])\n",
    "\n",
    "with tf.name_scope('fc1') as scope:\n",
    "    weights =  tf.Variable(tf.truncated_normal([input_length, FC1_SIZE], dtype=tf.float32, stddev=0.1, seed=SEED))\n",
    "    fc = tf.matmul(input_flat,weights)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[FC1_SIZE]))\n",
    "    bias = tf.nn.bias_add(fc, biases)\n",
    "    fc1 = tf.nn.relu(bias, name=scope)\n",
    "    keep_prob1 = tf.placeholder(tf.float32)\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob1, name=scope)\n",
    "    #fc1_drop = fc1\n",
    "    \n",
    "    fc1_weights = weights\n",
    "    fc1_biases = biases\n",
    "    \n",
    "with tf.name_scope('fc2') as scope:\n",
    "    weights =  tf.Variable(tf.truncated_normal([FC1_SIZE, FC2_SIZE], dtype=tf.float32, stddev=0.1, seed=SEED))\n",
    "    fc = tf.matmul(fc1_drop,weights)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[FC2_SIZE]))\n",
    "    bias = tf.nn.bias_add(fc, biases)\n",
    "    fc2 = tf.nn.relu(bias, name=scope)\n",
    "    keep_prob2 = tf.placeholder(tf.float32)\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob2, name=scope)\n",
    "    #fc1_drop = fc1\n",
    "    \n",
    "    fc2_weights = weights\n",
    "    fc2_biases = biases\n",
    "    \n",
    "with tf.name_scope('fc3') as scope:\n",
    "    weights =  tf.Variable(tf.truncated_normal([FC2_SIZE, FC3_SIZE], dtype=tf.float32, stddev=0.1, seed=SEED))\n",
    "    fc = tf.matmul(fc2_drop,weights)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[FC3_SIZE]))\n",
    "    bias = tf.nn.bias_add(fc, biases)\n",
    "    fc3 = tf.nn.relu(bias, name=scope)\n",
    "    keep_prob3 = tf.placeholder(tf.float32)\n",
    "    fc3_drop = tf.nn.dropout(fc3, keep_prob3, name=scope)\n",
    "    #fc1_drop = fc1\n",
    "    \n",
    "    fc3_weights = weights\n",
    "    fc3_biases = biases\n",
    "    \n",
    "with tf.name_scope('fc4') as scope:\n",
    "    weights =  tf.Variable(tf.truncated_normal([FC3_SIZE, FC4_SIZE], dtype=tf.float32, stddev=0.1, seed=SEED))\n",
    "    fc = tf.matmul(fc3_drop,weights)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[FC4_SIZE]))\n",
    "    bias = tf.nn.bias_add(fc, biases)\n",
    "    fc4 = tf.nn.relu(bias, name=scope)\n",
    "    keep_prob4 = tf.placeholder(tf.float32)\n",
    "    fc4_drop = tf.nn.dropout(fc4, keep_prob3, name=scope)\n",
    "    #fc1_drop = fc1\n",
    "    \n",
    "    fc4_weights = weights\n",
    "    fc4_biases = biases\n",
    "    \n",
    "    \n",
    "with tf.name_scope('fc5') as scope:\n",
    "    weights =  tf.Variable(tf.truncated_normal([FC4_SIZE, NUM_LABELS], dtype=tf.float32, stddev=0.1, seed=SEED))\n",
    "    fc = tf.matmul(fc4_drop,weights)\n",
    "    biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "    bias = tf.nn.bias_add(fc, biases)\n",
    "    #fc2 = tf.nn.relu(bias, name=scope)\n",
    "    fc5 = bias\n",
    "    out_y = fc5\n",
    "    \n",
    "    fc5_weights = weights\n",
    "    fc5_biases = biases\n",
    "    \n",
    "        \n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out_y, true_y))\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(out_y, true_y))\n",
    "\n",
    "#cross_entropy = -tf.reduce_sum(true_y*tf.log(out_y))\n",
    "#loss = cross_entropy\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers =  (\n",
    "                tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases) +\n",
    "                tf.nn.l2_loss(fc3_weights) + tf.nn.l2_loss(fc3_biases) +\n",
    "                tf.nn.l2_loss(fc4_weights) + tf.nn.l2_loss(fc4_biases)\n",
    "                #tf.nn.l2_loss(fc4_weights) + tf.nn.l2_loss(fc4_biases)\n",
    "                #tf.nn.l2_loss(conv1_weights) + tf.nn.l2_loss(conv1_biases) +\n",
    "                #tf.nn.l2_loss(conv2_weights) + tf.nn.l2_loss(conv2_biases)\n",
    ")\n",
    "# Add the regularization term to the loss.\n",
    "loss += REGULARIZER_SIZE * (regularizers)\n",
    "\n",
    "\n",
    "#For one-label\n",
    "#correct_prediction = tf.equal(tf.argmax(out_y,1), tf.argmax(true_y,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#accuracy2 = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#For multi-label\n",
    "#see http://stackoverflow.com/questions/37746670/tensorflow-multi-label-accuracy-calculation\n",
    "correct_prediction = tf.equal(tf.round(tf.nn.sigmoid(out_y)), tf.round(true_y))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#Accuracy where all labels need to be correct:\n",
    "all_labels_true = tf.reduce_min(tf.cast(correct_prediction, tf.float32), 1)\n",
    "accuracy2 = tf.reduce_mean(all_labels_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_step = tf.train.GradientDescentOptimizer(1e-5).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data_size,batch_size,first_batch=False):\n",
    "    global ind\n",
    "    try:\n",
    "        ind += 0\n",
    "    except:\n",
    "        first_batch = True\n",
    "    \n",
    "    if first_batch == True or batch_size != len(ind):\n",
    "        ind = np.mod(range(batch_size),data_size)\n",
    "        return ind\n",
    "    \n",
    "    ind += batch_size\n",
    "    ind %= data_size\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization:\n",
      "    fc1 norm   63.7157, grad norm 63.7157, bias norm 2.26274, bias grad norm 2.26274\n",
      "    fc2 norm   31.9248, grad norm 31.9248, bias norm 1.6, bias grad norm 1.6\n",
      "    fc3 norm   15.9852, grad norm 15.9852, bias norm 1.13137, bias grad norm 1.13137\n",
      "    fc4 norm   7.88883, grad norm 7.88883, bias norm 0.8, bias grad norm 0.8\n",
      "    fc5 norm   2.29394, grad norm 2.29394, bias norm 0.34641, bias grad norm 0.34641\n",
      "    out_y_norm 22.3004, grad norm 0.00269108\n",
      "    training accuracy 0.305472, valid'n accuracy 0.302971, loss 0.741645\n",
      "    training accuracy2 0, valid'n accuracy2 0\n",
      "Training time for 1000 steps: 199.008\n",
      "Step 1:\n",
      "    fc1 norm   63.474, grad norm 63.474, bias norm 2.25976, bias grad norm 2.25976\n",
      "    fc2 norm   31.9193, grad norm 31.9193, bias norm 1.59734, bias grad norm 1.59734\n",
      "    fc3 norm   15.982, grad norm 15.982, bias norm 1.129, bias grad norm 1.129\n",
      "    fc4 norm   7.88665, grad norm 7.88665, bias norm 0.799033, bias grad norm 0.799033\n",
      "    fc5 norm   2.29413, grad norm 2.29413, bias norm 0.342946, bias grad norm 0.342946\n",
      "    out_y_norm 17.9144, grad norm 0.00261295\n",
      "    training accuracy 0.583194, valid'n accuracy 0.580946, loss 0.711594\n",
      "    training accuracy2 0, valid'n accuracy2 0\n",
      "    seconds since start 6.22907, ETA(s) 622901\n",
      "Step 100:\n",
      "    fc1 norm   29.6073, grad norm 29.6073, bias norm 1.96853, bias grad norm 1.96853\n",
      "    fc2 norm   30.2146, grad norm 30.2146, bias norm 1.45098, bias grad norm 1.45098\n",
      "    fc3 norm   15.0197, grad norm 15.0197, bias norm 1.24722, bias grad norm 1.24722\n",
      "    fc4 norm   7.78206, grad norm 7.78206, bias norm 0.954141, bias grad norm 0.954141\n",
      "    fc5 norm   2.35869, grad norm 2.35869, bias norm 0.177517, bias grad norm 0.177517\n",
      "    out_y_norm 159.048, grad norm 0.00233527\n",
      "    training accuracy 0.733889, valid'n accuracy 0.735046, loss 0.592284\n",
      "    training accuracy2 0, valid'n accuracy2 0\n",
      "    seconds since start 29.1522, ETA(s) 29123\n",
      "Step 200:\n",
      "    fc1 norm   24.2928, grad norm 24.2928, bias norm 1.55426, bias grad norm 1.55426\n",
      "    fc2 norm   28.0837, grad norm 28.0837, bias norm 1.39612, bias grad norm 1.39612\n",
      "    fc3 norm   14.1139, grad norm 14.1139, bias norm 1.22742, bias grad norm 1.22742\n",
      "    fc4 norm   7.82912, grad norm 7.82912, bias norm 1.0942, bias grad norm 1.0942\n",
      "    fc5 norm   2.42336, grad norm 2.42336, bias norm 0.0902919, bias grad norm 0.0902919\n",
      "    out_y_norm 186.282, grad norm 0.00222433\n",
      "    training accuracy 0.731611, valid'n accuracy 0.735046, loss 0.544704\n",
      "    training accuracy2 0, valid'n accuracy2 0\n",
      "    seconds since start 52.5745, ETA(s) 26234.7\n",
      "Step 300:\n",
      "    fc1 norm   37.7627, grad norm 37.7627, bias norm 0.715645, bias grad norm 0.715645\n",
      "    fc2 norm   28.5861, grad norm 28.5861, bias norm 1.44973, bias grad norm 1.44973\n",
      "    fc3 norm   15.8574, grad norm 15.8574, bias norm 1.33976, bias grad norm 1.33976\n",
      "    fc4 norm   9.53671, grad norm 9.53671, bias norm 1.1278, bias grad norm 1.1278\n",
      "    fc5 norm   2.98662, grad norm 2.98662, bias norm 0.0706716, bias grad norm 0.0706716\n",
      "    out_y_norm 595.293, grad norm 0.00158694\n",
      "    training accuracy 0.866861, valid'n accuracy 0.860401, loss 0.304275\n",
      "    training accuracy2 0.168, valid'n accuracy2 0.150095\n",
      "    seconds since start 77.0082, ETA(s) 25592.4\n",
      "Step 400:\n",
      "    fc1 norm   36.5609, grad norm 36.5609, bias norm 0.610163, bias grad norm 0.610163\n",
      "    fc2 norm   29.1964, grad norm 29.1964, bias norm 1.43072, bias grad norm 1.43072\n",
      "    fc3 norm   17.4861, grad norm 17.4861, bias norm 1.22663, bias grad norm 1.22663\n",
      "    fc4 norm   10.5799, grad norm 10.5799, bias norm 1.11583, bias grad norm 1.11583\n",
      "    fc5 norm   3.37074, grad norm 3.37074, bias norm 0.0952679, bias grad norm 0.0952679\n",
      "    out_y_norm 762.924, grad norm 0.00150226\n",
      "    training accuracy 0.883528, valid'n accuracy 0.880307, loss 0.271545\n",
      "    training accuracy2 0.199667, valid'n accuracy2 0.202652\n",
      "    seconds since start 100.177, ETA(s) 24944.2\n",
      "Step 500:\n",
      "    fc1 norm   37.0937, grad norm 37.0937, bias norm 0.559059, bias grad norm 0.559059\n",
      "    fc2 norm   30.4214, grad norm 30.4214, bias norm 1.37344, bias grad norm 1.37344\n",
      "    fc3 norm   19.1886, grad norm 19.1886, bias norm 1.16596, bias grad norm 1.16596\n",
      "    fc4 norm   11.5831, grad norm 11.5831, bias norm 1.1376, bias grad norm 1.1376\n",
      "    fc5 norm   3.72514, grad norm 3.72514, bias norm 0.127842, bias grad norm 0.127842\n",
      "    out_y_norm 874.08, grad norm 0.00116204\n",
      "    training accuracy 0.936083, valid'n accuracy 0.932134, loss 0.184728\n",
      "    training accuracy2 0.482667, valid'n accuracy2 0.472064\n",
      "    seconds since start 123.728, ETA(s) 24622\n",
      "Step 600:\n",
      "    fc1 norm   36.8563, grad norm 36.8563, bias norm 0.518131, bias grad norm 0.518131\n",
      "    fc2 norm   31.0107, grad norm 31.0107, bias norm 1.3341, bias grad norm 1.3341\n",
      "    fc3 norm   20.4623, grad norm 20.4623, bias norm 1.08413, bias grad norm 1.08413\n",
      "    fc4 norm   12.4611, grad norm 12.4611, bias norm 1.08998, bias grad norm 1.08998\n",
      "    fc5 norm   4.05609, grad norm 4.05609, bias norm 0.139436, bias grad norm 0.139436\n",
      "    out_y_norm 937.71, grad norm 0.00111062\n",
      "    training accuracy 0.939389, valid'n accuracy 0.938368, loss 0.168762\n",
      "    training accuracy2 0.518, valid'n accuracy2 0.519176\n",
      "    seconds since start 147.995, ETA(s) 24517.8\n",
      "Step 700:\n",
      "    fc1 norm   36.8966, grad norm 36.8966, bias norm 0.4878, bias grad norm 0.4878\n",
      "    fc2 norm   31.6285, grad norm 31.6285, bias norm 1.30193, bias grad norm 1.30193\n",
      "    fc3 norm   21.606, grad norm 21.606, bias norm 1.05065, bias grad norm 1.05065\n",
      "    fc4 norm   13.246, grad norm 13.246, bias norm 1.09353, bias grad norm 1.09353\n",
      "    fc5 norm   4.39903, grad norm 4.39903, bias norm 0.156287, bias grad norm 0.156287\n",
      "    out_y_norm 1040.01, grad norm 0.00100543\n",
      "    training accuracy 0.950194, valid'n accuracy 0.943379, loss 0.143494\n",
      "    training accuracy2 0.575667, valid'n accuracy2 0.54143\n",
      "    seconds since start 174.419, ETA(s) 24742.5\n",
      "Step 800:\n",
      "    fc1 norm   37.0528, grad norm 37.0528, bias norm 0.465345, bias grad norm 0.465345\n",
      "    fc2 norm   32.2865, grad norm 32.2865, bias norm 1.27566, bias grad norm 1.27566\n",
      "    fc3 norm   22.6758, grad norm 22.6758, bias norm 1.04917, bias grad norm 1.04917\n",
      "    fc4 norm   14.0091, grad norm 14.0091, bias norm 1.13101, bias grad norm 1.13101\n",
      "    fc5 norm   4.75685, grad norm 4.75685, bias norm 0.175984, bias grad norm 0.175984\n",
      "    out_y_norm 1165.53, grad norm 0.000940957\n",
      "    training accuracy 0.9565, valid'n accuracy 0.953086, loss 0.128946\n",
      "    training accuracy2 0.621333, valid'n accuracy2 0.611742\n",
      "    seconds since start 200.221, ETA(s) 24827.4\n",
      "Step 900:\n",
      "    fc1 norm   37.407, grad norm 37.407, bias norm 0.444234, bias grad norm 0.444234\n",
      "    fc2 norm   33.0146, grad norm 33.0146, bias norm 1.2526, bias grad norm 1.2526\n",
      "    fc3 norm   23.7485, grad norm 23.7485, bias norm 1.03963, bias grad norm 1.03963\n",
      "    fc4 norm   14.7841, grad norm 14.7841, bias norm 1.18439, bias grad norm 1.18439\n",
      "    fc5 norm   5.13345, grad norm 5.13345, bias norm 0.200799, bias grad norm 0.200799\n",
      "    out_y_norm 1273.76, grad norm 0.000856619\n",
      "    training accuracy 0.964889, valid'n accuracy 0.962476, loss 0.113107\n",
      "    training accuracy2 0.698667, valid'n accuracy2 0.683712\n",
      "    seconds since start 224.872, ETA(s) 24760.9\n",
      "Step 1000:\n",
      "    fc1 norm   37.8753, grad norm 37.8753, bias norm 0.425853, bias grad norm 0.425853\n",
      "    fc2 norm   33.8278, grad norm 33.8278, bias norm 1.23548, bias grad norm 1.23548\n",
      "    fc3 norm   24.862, grad norm 24.862, bias norm 1.03217, bias grad norm 1.03217\n",
      "    fc4 norm   15.5877, grad norm 15.5877, bias norm 1.24988, bias grad norm 1.24988\n",
      "    fc5 norm   5.52827, grad norm 5.52827, bias norm 0.228723, bias grad norm 0.228723\n",
      "    out_y_norm 1399.4, grad norm 0.000782014\n",
      "    training accuracy 0.971639, valid'n accuracy 0.969934, loss 0.100686\n",
      "    training accuracy2 0.762, valid'n accuracy2 0.747633\n",
      "    seconds since start 250.902, ETA(s) 24839.3\n",
      "Step 1100:\n",
      "    fc1 norm   38.4378, grad norm 38.4378, bias norm 0.410878, bias grad norm 0.410878\n",
      "    fc2 norm   34.6837, grad norm 34.6837, bias norm 1.21848, bias grad norm 1.21848\n",
      "    fc3 norm   25.9653, grad norm 25.9653, bias norm 1.0298, bias grad norm 1.0298\n",
      "    fc4 norm   16.3988, grad norm 16.3988, bias norm 1.32206, bias grad norm 1.32206\n",
      "    fc5 norm   5.92281, grad norm 5.92281, bias norm 0.261858, bias grad norm 0.261858\n",
      "    out_y_norm 1567.82, grad norm 0.000665616\n",
      "    training accuracy 0.981139, valid'n accuracy 0.976937, loss 0.0835038\n",
      "    training accuracy2 0.823333, valid'n accuracy2 0.799479\n",
      "    seconds since start 277.896, ETA(s) 24985.4\n",
      "Step 1200:\n",
      "    fc1 norm   38.9425, grad norm 38.9425, bias norm 0.396611, bias grad norm 0.396611\n",
      "    fc2 norm   35.5247, grad norm 35.5247, bias norm 1.2075, bias grad norm 1.2075\n",
      "    fc3 norm   27.0403, grad norm 27.0403, bias norm 1.0344, bias grad norm 1.0344\n",
      "    fc4 norm   17.2053, grad norm 17.2053, bias norm 1.40229, bias grad norm 1.40229\n",
      "    fc5 norm   6.32492, grad norm 6.32492, bias norm 0.29927, bias grad norm 0.29927\n",
      "    out_y_norm 1735.52, grad norm 0.000576977\n",
      "    training accuracy 0.987917, valid'n accuracy 0.985795, loss 0.0718731\n",
      "    training accuracy2 0.881333, valid'n accuracy2 0.868608\n",
      "    seconds since start 304.632, ETA(s) 25081.4\n",
      "Step 1300:\n",
      "    fc1 norm   39.4239, grad norm 39.4239, bias norm 0.383298, bias grad norm 0.383298\n",
      "    fc2 norm   36.2894, grad norm 36.2894, bias norm 1.19475, bias grad norm 1.19475\n",
      "    fc3 norm   28.067, grad norm 28.067, bias norm 1.03226, bias grad norm 1.03226\n",
      "    fc4 norm   17.9827, grad norm 17.9827, bias norm 1.48993, bias grad norm 1.48993\n",
      "    fc5 norm   6.71975, grad norm 6.71975, bias norm 0.342536, bias grad norm 0.342536\n",
      "    out_y_norm 1877.7, grad norm 0.000512958\n",
      "    training accuracy 0.991722, valid'n accuracy 0.989643, loss 0.0646544\n",
      "    training accuracy2 0.916667, valid'n accuracy2 0.906723\n",
      "    seconds since start 330.592, ETA(s) 25099.6\n",
      "Step 1400:\n",
      "    fc1 norm   39.8309, grad norm 39.8309, bias norm 0.373191, bias grad norm 0.373191\n",
      "    fc2 norm   36.9748, grad norm 36.9748, bias norm 1.18707, bias grad norm 1.18707\n",
      "    fc3 norm   29.031, grad norm 29.031, bias norm 1.0366, bias grad norm 1.0366\n",
      "    fc4 norm   18.7421, grad norm 18.7421, bias norm 1.58204, bias grad norm 1.58204\n",
      "    fc5 norm   7.11103, grad norm 7.11103, bias norm 0.390085, bias grad norm 0.390085\n",
      "    out_y_norm 2018.62, grad norm 0.000464453\n",
      "    training accuracy 0.993111, valid'n accuracy 0.993233, loss 0.0591016\n",
      "    training accuracy2 0.936, valid'n accuracy2 0.941998\n",
      "    seconds since start 358.902, ETA(s) 25277\n",
      "Step 1500:\n",
      "    fc1 norm   40.1997, grad norm 40.1997, bias norm 0.361986, bias grad norm 0.361986\n",
      "    fc2 norm   37.6292, grad norm 37.6292, bias norm 1.17367, bias grad norm 1.17367\n",
      "    fc3 norm   29.9401, grad norm 29.9401, bias norm 1.04427, bias grad norm 1.04427\n",
      "    fc4 norm   19.4977, grad norm 19.4977, bias norm 1.66398, bias grad norm 1.66398\n",
      "    fc5 norm   7.50154, grad norm 7.50154, bias norm 0.442098, bias grad norm 0.442098\n",
      "    out_y_norm 2190.09, grad norm 0.000380353\n",
      "    training accuracy 0.995917, valid'n accuracy 0.995561, loss 0.0503476\n",
      "    training accuracy2 0.957667, valid'n accuracy2 0.956439\n",
      "    seconds since start 386.06, ETA(s) 25351.3\n",
      "Step 1600:\n",
      "    fc1 norm   40.4492, grad norm 40.4492, bias norm 0.35294, bias grad norm 0.35294\n",
      "    fc2 norm   38.1462, grad norm 38.1462, bias norm 1.16153, bias grad norm 1.16153\n",
      "    fc3 norm   30.7524, grad norm 30.7524, bias norm 1.04998, bias grad norm 1.04998\n",
      "    fc4 norm   20.2135, grad norm 20.2135, bias norm 1.7345, bias grad norm 1.7345\n",
      "    fc5 norm   7.88021, grad norm 7.88021, bias norm 0.492493, bias grad norm 0.492493\n",
      "    out_y_norm 2324.94, grad norm 0.00031827\n",
      "    training accuracy 0.997972, valid'n accuracy 0.997199, loss 0.0457389\n",
      "    training accuracy2 0.979, valid'n accuracy2 0.971828\n",
      "    seconds since start 413.147, ETA(s) 25408.5\n",
      "Step 1700:\n",
      "    fc1 norm   40.6829, grad norm 40.6829, bias norm 0.343433, bias grad norm 0.343433\n",
      "    fc2 norm   38.5401, grad norm 38.5401, bias norm 1.1528, bias grad norm 1.1528\n",
      "    fc3 norm   31.4873, grad norm 31.4873, bias norm 1.05873, bias grad norm 1.05873\n",
      "    fc4 norm   20.8991, grad norm 20.8991, bias norm 1.78677, bias grad norm 1.78677\n",
      "    fc5 norm   8.25078, grad norm 8.25078, bias norm 0.542951, bias grad norm 0.542951\n",
      "    out_y_norm 2496.92, grad norm 0.000283889\n",
      "    training accuracy 0.998306, valid'n accuracy 0.997869, loss 0.0427915\n",
      "    training accuracy2 0.982667, valid'n accuracy2 0.97822\n",
      "    seconds since start 441.004, ETA(s) 25500.4\n",
      "Step 1800:\n",
      "    fc1 norm   40.7883, grad norm 40.7883, bias norm 0.334659, bias grad norm 0.334659\n",
      "    fc2 norm   38.8043, grad norm 38.8043, bias norm 1.14053, bias grad norm 1.14053\n",
      "    fc3 norm   32.1193, grad norm 32.1193, bias norm 1.06297, bias grad norm 1.06297\n",
      "    fc4 norm   21.5336, grad norm 21.5336, bias norm 1.84044, bias grad norm 1.84044\n",
      "    fc5 norm   8.60714, grad norm 8.60714, bias norm 0.593539, bias grad norm 0.593539\n",
      "    out_y_norm 2616.66, grad norm 0.000265972\n",
      "    training accuracy 0.998389, valid'n accuracy 0.998363, loss 0.0408543\n",
      "    training accuracy2 0.983667, valid'n accuracy2 0.982244\n",
      "    seconds since start 468.227, ETA(s) 25544.4\n",
      "Step 1900:\n",
      "    fc1 norm   40.8405, grad norm 40.8405, bias norm 0.325437, bias grad norm 0.325437\n",
      "    fc2 norm   38.9737, grad norm 38.9737, bias norm 1.13282, bias grad norm 1.13282\n",
      "    fc3 norm   32.6737, grad norm 32.6737, bias norm 1.07301, bias grad norm 1.07301\n",
      "    fc4 norm   22.1366, grad norm 22.1366, bias norm 1.88554, bias grad norm 1.88554\n",
      "    fc5 norm   8.95838, grad norm 8.95838, bias norm 0.646924, bias grad norm 0.646924\n",
      "    out_y_norm 2789.18, grad norm 0.000226149\n",
      "    training accuracy 0.999, valid'n accuracy 0.99858, loss 0.0381988\n",
      "    training accuracy2 0.988, valid'n accuracy2 0.983191\n",
      "    seconds since start 497.856, ETA(s) 25705.1\n",
      "Step 2000:\n",
      "    fc1 norm   40.8496, grad norm 40.8496, bias norm 0.318907, bias grad norm 0.318907\n",
      "    fc2 norm   39.0511, grad norm 39.0511, bias norm 1.11895, bias grad norm 1.11895\n",
      "    fc3 norm   33.1562, grad norm 33.1562, bias norm 1.07555, bias grad norm 1.07555\n",
      "    fc4 norm   22.6944, grad norm 22.6944, bias norm 1.92763, bias grad norm 1.92763\n",
      "    fc5 norm   9.30041, grad norm 9.30041, bias norm 0.698848, bias grad norm 0.698848\n",
      "    out_y_norm 2852.99, grad norm 0.000192511\n",
      "    training accuracy 0.9995, valid'n accuracy 0.999053, loss 0.0365526\n",
      "    training accuracy2 0.994, valid'n accuracy2 0.988636\n",
      "    seconds since start 526.127, ETA(s) 25780.2\n",
      "Step 2100:\n",
      "    fc1 norm   40.798, grad norm 40.798, bias norm 0.311669, bias grad norm 0.311669\n",
      "    fc2 norm   39.0583, grad norm 39.0583, bias norm 1.11001, bias grad norm 1.11001\n",
      "    fc3 norm   33.5585, grad norm 33.5585, bias norm 1.08538, bias grad norm 1.08538\n",
      "    fc4 norm   23.2286, grad norm 23.2286, bias norm 1.95817, bias grad norm 1.95817\n",
      "    fc5 norm   9.6377, grad norm 9.6377, bias norm 0.752685, bias grad norm 0.752685\n",
      "    out_y_norm 2955.17, grad norm 0.000165643\n",
      "    training accuracy 0.999778, valid'n accuracy 0.999665, loss 0.0351433\n",
      "    training accuracy2 0.997333, valid'n accuracy2 0.995975\n",
      "    seconds since start 551.592, ETA(s) 25714.7\n",
      "Step 2200:\n",
      "    fc1 norm   40.7084, grad norm 40.7084, bias norm 0.304785, bias grad norm 0.304785\n",
      "    fc2 norm   38.9868, grad norm 38.9868, bias norm 1.09567, bias grad norm 1.09567\n",
      "    fc3 norm   33.8872, grad norm 33.8872, bias norm 1.09389, bias grad norm 1.09389\n",
      "    fc4 norm   23.7259, grad norm 23.7259, bias norm 1.98891, bias grad norm 1.98891\n",
      "    fc5 norm   9.96756, grad norm 9.96756, bias norm 0.806141, bias grad norm 0.806141\n",
      "    out_y_norm 3073.94, grad norm 0.000159948\n",
      "    training accuracy 0.999778, valid'n accuracy 0.999842, loss 0.0345189\n",
      "    training accuracy2 0.997333, valid'n accuracy2 0.998106\n",
      "    seconds since start 576.965, ETA(s) 25648.7\n",
      "Step 2300:\n",
      "    fc1 norm   40.5481, grad norm 40.5481, bias norm 0.296848, bias grad norm 0.296848\n",
      "    fc2 norm   38.8352, grad norm 38.8352, bias norm 1.08352, bias grad norm 1.08352\n",
      "    fc3 norm   34.1385, grad norm 34.1385, bias norm 1.10088, bias grad norm 1.10088\n",
      "    fc4 norm   24.1889, grad norm 24.1889, bias norm 2.01865, bias grad norm 2.01865\n",
      "    fc5 norm   10.292, grad norm 10.292, bias norm 0.859485, bias grad norm 0.859485\n",
      "    out_y_norm 3184.07, grad norm 0.000130061\n",
      "    training accuracy 0.999917, valid'n accuracy 0.999941, loss 0.0328052\n",
      "    training accuracy2 0.999, valid'n accuracy2 0.99929\n",
      "    seconds since start 603.131, ETA(s) 25620\n",
      "Step 2400:\n",
      "    fc1 norm   40.3909, grad norm 40.3909, bias norm 0.289637, bias grad norm 0.289637\n",
      "    fc2 norm   38.689, grad norm 38.689, bias norm 1.06929, bias grad norm 1.06929\n",
      "    fc3 norm   34.3398, grad norm 34.3398, bias norm 1.10972, bias grad norm 1.10972\n",
      "    fc4 norm   24.6225, grad norm 24.6225, bias norm 2.04479, bias grad norm 2.04479\n",
      "    fc5 norm   10.6143, grad norm 10.6143, bias norm 0.91399, bias grad norm 0.91399\n",
      "    out_y_norm 3331.91, grad norm 0.0001169\n",
      "    training accuracy 0.999972, valid'n accuracy 0.99998, loss 0.0319111\n",
      "    training accuracy2 0.999667, valid'n accuracy2 0.999763\n",
      "    seconds since start 629.223, ETA(s) 25588.4\n",
      "Step 2500:\n",
      "    fc1 norm   40.1866, grad norm 40.1866, bias norm 0.284556, bias grad norm 0.284556\n",
      "    fc2 norm   38.4968, grad norm 38.4968, bias norm 1.05694, bias grad norm 1.05694\n",
      "    fc3 norm   34.4996, grad norm 34.4996, bias norm 1.11415, bias grad norm 1.11415\n",
      "    fc4 norm   25.0232, grad norm 25.0232, bias norm 2.06467, bias grad norm 2.06467\n",
      "    fc5 norm   10.934, grad norm 10.934, bias norm 0.969732, bias grad norm 0.969732\n",
      "    out_y_norm 3358.83, grad norm 0.000108081\n",
      "    training accuracy 1, valid'n accuracy 1, loss 0.0316229\n",
      "    training accuracy2 1, valid'n accuracy2 1\n",
      "    seconds since start 654.313, ETA(s) 25518.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f2a960ce6b05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training time for 1000 steps: %g\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stop\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kayote/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kayote/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kayote/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kayote/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kayote/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "batch_size = BATCH_SIZE\n",
    "layer_diagnostics = pd.DataFrame(columns=('step','layer',\n",
    "                                          'weight','weight_grad','bias','bias_grad'#,\n",
    "                                          #'weight_norm','weight_grad_norm','bias_norm','bias_grad_norm'\n",
    "                                         ))\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "for i in range(MAX_STEPS+1):\n",
    "    ind = next_batch(train_x.shape[0],batch_size)\n",
    "    batch_x = train_x[ind]\n",
    "    batch_y = train_y[ind]\n",
    "    if i > 0: \n",
    "        if i<2: train_start = timeit.default_timer()\n",
    "        sess.run(train_step, feed_dict={inputs:batch_x,true_y:batch_y, keep_prob1:0.5, keep_prob2:0.5, keep_prob3:0.5})\n",
    "        if i<2: train_stop = timeit.default_timer()\n",
    "        if i<2: print(\"Training time for 1000 steps: %g\"%(1000*(train_stop-train_start)))\n",
    "    if i < 2 or ( i % STEP_UPDATE == 0 and i != 0 ) :\n",
    "        \n",
    "        fc1_weights_val, fc1_weights_grad_val,\\\n",
    "        fc1_biases_val, fc1_biases_grad_val,\\\n",
    "        fc2_weights_val, fc2_weights_grad_val,\\\n",
    "        fc2_biases_val, fc2_biases_grad_val,\\\n",
    "        fc3_weights_val, fc3_weights_grad_val,\\\n",
    "        fc3_biases_val, fc3_biases_grad_val,\\\n",
    "        fc4_weights_val, fc4_weights_grad_val,\\\n",
    "        fc4_biases_val, fc4_biases_grad_val,\\\n",
    "        fc5_weights_val, fc5_weights_grad_val,\\\n",
    "        fc5_biases_val, fc5_biases_grad_val,\\\n",
    "        out_y_val, out_y_grad_val,\\\n",
    "        trainAccuracy, trainAccuracy2, trainLoss =\\\n",
    "            sess.run([\n",
    "                fc1_weights, tf.gradients(loss,fc1_weights)[0],\n",
    "                fc1_biases, tf.gradients(loss,fc1_biases)[0],\n",
    "                fc2_weights, tf.gradients(loss,fc2_weights)[0],\n",
    "                fc2_biases, tf.gradients(loss,fc2_biases)[0],\n",
    "                fc3_weights, tf.gradients(loss,fc3_weights)[0],\n",
    "                fc3_biases, tf.gradients(loss,fc3_biases)[0],\n",
    "                fc4_weights, tf.gradients(loss,fc4_weights)[0],\n",
    "                fc4_biases, tf.gradients(loss,fc4_biases)[0],\n",
    "                fc5_weights, tf.gradients(loss,fc5_weights)[0],\n",
    "                fc5_biases, tf.gradients(loss,fc5_biases)[0],\n",
    "                out_y, tf.gradients(loss,out_y)[0],\n",
    "                accuracy, accuracy2, loss\n",
    "            ],\n",
    "                                feed_dict={\n",
    "                                    inputs:batch_x,\n",
    "                                    true_y:batch_y,\n",
    "                                    keep_prob1:1.0,\n",
    "                                    keep_prob2:1.0,\n",
    "                                    keep_prob3:1.0,\n",
    "                                    keep_prob4:1.0})\n",
    "        \n",
    "        #validationAccuracy = sess.run(accuracy, \n",
    "        #                        feed_dict={\n",
    "        #                            inputs:validation_x,\n",
    "        #                            true_y:validation_y, \n",
    "        #                            keep_prob:1.0})\n",
    "        \n",
    "        fc1_norm = np.linalg.norm(fc1_weights_val)\n",
    "        fc1_grad_norm = np.linalg.norm(fc1_weights_val)\n",
    "        fc1_bias_norm = np.linalg.norm(fc1_biases_val)\n",
    "        fc1_bias_grad_norm = np.linalg.norm(fc1_biases_val)\n",
    "        fc2_norm = np.linalg.norm(fc2_weights_val)\n",
    "        fc2_grad_norm = np.linalg.norm(fc2_weights_val)\n",
    "        fc2_bias_norm = np.linalg.norm(fc2_biases_val)\n",
    "        fc2_bias_grad_norm = np.linalg.norm(fc2_biases_val)\n",
    "        fc3_norm = np.linalg.norm(fc3_weights_val)\n",
    "        fc3_grad_norm = np.linalg.norm(fc3_weights_val)\n",
    "        fc3_bias_norm = np.linalg.norm(fc3_biases_val)\n",
    "        fc3_bias_grad_norm = np.linalg.norm(fc3_biases_val)\n",
    "        fc4_norm = np.linalg.norm(fc4_weights_val)\n",
    "        fc4_grad_norm = np.linalg.norm(fc4_weights_val)\n",
    "        fc4_bias_norm = np.linalg.norm(fc4_biases_val)\n",
    "        fc4_bias_grad_norm = np.linalg.norm(fc4_biases_val)\n",
    "        fc5_norm = np.linalg.norm(fc5_weights_val)\n",
    "        fc5_grad_norm = np.linalg.norm(fc5_weights_val)\n",
    "        fc5_bias_norm = np.linalg.norm(fc5_biases_val)\n",
    "        fc5_bias_grad_norm = np.linalg.norm(fc5_biases_val)\n",
    "        out_y_norm = np.linalg.norm(out_y_val)\n",
    "        out_y_grad_norm = np.linalg.norm(out_y_grad_val)\n",
    "        \n",
    "        \n",
    "        layer_diagnostics.append({\n",
    "                'step':i,\n",
    "                'layer':'fc1',\n",
    "                'weight_norm':fc1_norm,\n",
    "                'weight_grad_norm':fc1_grad_norm,\n",
    "                'bias_norm':fc1_bias_norm,\n",
    "                'bias_grad_norm':fc1_bias_grad_norm},ignore_index=True)\n",
    "        layer_diagnostics.append({\n",
    "                'step':i,\n",
    "                'layer':'fc2',\n",
    "                'weight_norm':fc2_norm,\n",
    "                'weight_grad_norm':fc2_grad_norm,\n",
    "                'bias_norm':fc2_bias_norm,\n",
    "                'bias_grad_norm':fc2_bias_grad_norm},ignore_index=True)\n",
    "        layer_diagnostics.append({\n",
    "                'step':i,\n",
    "                'layer':'fc3',\n",
    "                'weight_norm':fc3_norm,\n",
    "                'weight_grad_norm':fc3_grad_norm,\n",
    "                'bias_norm':fc3_bias_norm,\n",
    "                'bias_grad_norm':fc3_bias_grad_norm},ignore_index=True)\n",
    "        layer_diagnostics.append({\n",
    "                'step':i,\n",
    "                'layer':'fc4',\n",
    "                'weight_norm':fc4_norm,\n",
    "                'weight_grad_norm':fc4_grad_norm,\n",
    "                'bias_norm':fc4_bias_norm,\n",
    "                'bias_grad_norm':fc4_bias_grad_norm},ignore_index=True)\n",
    "        layer_diagnostics.append({\n",
    "                'step':i,\n",
    "                'layer':'fc5',\n",
    "                'weight_norm':fc5_norm,\n",
    "                'weight_grad_norm':fc5_grad_norm,\n",
    "                'bias_norm':fc5_bias_norm,\n",
    "                'bias_grad_norm':fc5_bias_grad_norm},ignore_index=True)\n",
    "        layer_diagnostics.append({\n",
    "                'step':i,\n",
    "                'layer':'out_y',\n",
    "                'weight_norm':out_y_norm,\n",
    "                'weight_grad_norm':out_y_grad_norm,\n",
    "                'bias_norm':0,\n",
    "                'bias_grad_norm':0},ignore_index=True)\n",
    "                \n",
    "        validationAccuracy, validationAccuracy2 = sess.run([accuracy, accuracy2], \n",
    "                            feed_dict={\n",
    "                                inputs:validation_x,\n",
    "                                true_y:validation_y,\n",
    "                                    keep_prob1:1.0,\n",
    "                                    keep_prob2:1.0,\n",
    "                                    keep_prob3:1.0,\n",
    "                                    keep_prob4:1.0})\n",
    "            \n",
    "                \n",
    "        stop = timeit.default_timer()\n",
    "\n",
    "        if i==0: print(\"Initialization:\")\n",
    "        else: print(\"Step %d:\"%(i))\n",
    "        print(\"    fc1 norm   %g, grad norm %g, bias norm %g, bias grad norm %g\"%\n",
    "              (fc1_norm, fc1_grad_norm, fc1_bias_norm, fc1_bias_grad_norm))\n",
    "        print(\"    fc2 norm   %g, grad norm %g, bias norm %g, bias grad norm %g\"%\n",
    "              (fc2_norm, fc2_grad_norm, fc2_bias_norm, fc2_bias_grad_norm))\n",
    "        print(\"    fc3 norm   %g, grad norm %g, bias norm %g, bias grad norm %g\"%\n",
    "              (fc3_norm, fc3_grad_norm, fc3_bias_norm, fc3_bias_grad_norm))\n",
    "        print(\"    fc4 norm   %g, grad norm %g, bias norm %g, bias grad norm %g\"%\n",
    "              (fc4_norm, fc4_grad_norm, fc4_bias_norm, fc4_bias_grad_norm))\n",
    "        print(\"    fc5 norm   %g, grad norm %g, bias norm %g, bias grad norm %g\"%\n",
    "              (fc5_norm, fc5_grad_norm, fc5_bias_norm, fc5_bias_grad_norm))\n",
    "        print(\"    out_y_norm %g, grad norm %g\"%(out_y_norm, out_y_grad_norm))\n",
    "        print(\"    training accuracy %g, valid'n accuracy %g, loss %g\"%(trainAccuracy, validationAccuracy,trainLoss))\n",
    "        print(\"    training accuracy2 %g, valid'n accuracy2 %g\"%(trainAccuracy2, validationAccuracy2))\n",
    "        #print(\"    percent training samples classified as not zero %g\"%(100.0*float(np.sum((np.argmax(out_y_val,1))))/float(out_y_val.shape[0])))\n",
    "        if i>0: print(\"    seconds since start %g, ETA(s) %g\"%((stop-start),(stop-start)*float(MAX_STEPS-i)/float(i)))\n",
    "            \n",
    "        \n",
    "        if trainAccuracy2>0.99:\n",
    "            print('Success! Shall stop training now although I could go further to improve loss function')\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Predictions:\n",
      "\n",
      "Incorrect Predictions:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#I want to get indices of train_x that are correctly, and incorrectly identified\n",
    "tmp1, tmp2 = sess.run([tf.nn.sigmoid(out_y), true_y], \n",
    "                            feed_dict={\n",
    "                                inputs:validation_x,\n",
    "                                true_y:validation_y, \n",
    "                                keep_prob1:1.0,keep_prob2:1.0,keep_prob3:1.0})\n",
    "\n",
    "print(\"Correct Predictions:\")\n",
    "for i in np.where(np.sum(np.abs(np.round(tmp1)-tmp2),1)==0)[0]:\n",
    "    0\n",
    "    #print('Guess vector: '+str(tmp1[i,:]))\n",
    "    #print('Guess: '+str(np.where(np.round(tmp1[i,:])==1)))\n",
    "    #tmp = np.power(validation_data['fft'].iloc[i],2.0)\n",
    "    #tmp = librosa.istft(np.repeat(tmp.reshape([len(tmp),1]),10,axis=1))\n",
    "    #IPython.display.display(IPython.display.Audio(data=tmp, rate=sr))\n",
    "print(\"\")    \n",
    "    \n",
    "print(\"Incorrect Predictions:\")\n",
    "for i in np.where(np.sum(np.abs(np.round(tmp1)-tmp2),1)!=0)[0]:\n",
    "    print(i)\n",
    "    #print('Guess vector: '+str(tmp1[i,:]))\n",
    "    print('Guess: '+str(np.where(np.round(tmp1[i,:])==1))+' Reality: '+str(np.where(tmp2[i,:]==1)))\n",
    "    #tmp = np.power(validation_data['fft'].iloc[i],2.0)\n",
    "    #tmp = librosa.istft(np.repeat(tmp.reshape([len(tmp),1]),10,axis=1))\n",
    "    #IPython.display.display(IPython.display.Audio(data=tmp, rate=sr))\n",
    "print(\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: (array([ 4,  8, 11]),) Reality: (array([ 4,  8, 11]),)\n",
      "Guess: (array([0, 5]),) Reality: (array([0, 5, 9]),)\n",
      "Guess: (array([1, 4, 9]),) Reality: (array([1, 4, 9]),)\n"
     ]
    }
   ],
   "source": [
    "#audio_path = librosa.util.example_audio_file()\n",
    "\n",
    "emaj, sr = librosa.load('emaj.aiff')\n",
    "amaj, sr = librosa.load('amaj.aiff')\n",
    "\n",
    "emaj_fft = get_fft(emaj)\n",
    "qmaj_fft = get_fft(librosa.effects.pitch_shift(emaj,sr,n_steps=1))\n",
    "amaj_fft = get_fft(amaj)\n",
    "emaj_fft = emaj_fft/np.sum(emaj_fft)\n",
    "qmaj_fft = qmaj_fft/np.sum(qmaj_fft)\n",
    "amaj_fft = amaj_fft/np.sum(amaj_fft)\n",
    "\n",
    "rwds = pd.DataFrame() #real-world dataset\n",
    "\n",
    "#http://www.electronics.dit.ie/staff/tscarff/Music_technology/midi/midi_note_numbers_for_octaves.htm\n",
    "emaj_dict = {'root':4,'majmin3':'maj','majmin7':'none','pitch_class':[4, 8, 11]}\n",
    "#emaj_dict.update({'target':label_to_chord.index(emaj_dict)})\n",
    "emaj_dict.update({'fft':emaj_fft})\n",
    "qmaj_dict = {'root':5,'majmin3':'maj','majmin7':'none','pitch_class':[0, 5, 9]}\n",
    "#qmaj_dict.update({'target':label_to_chord.index(qmaj_dict)})\n",
    "qmaj_dict.update({'fft':qmaj_fft})\n",
    "amaj_dict = {'root':9,'majmin3':'maj','majmin7':'none','pitch_class':[1, 4, 9]}\n",
    "#amaj_dict.update({'target':label_to_chord.index(amaj_dict)})\n",
    "amaj_dict.update({'fft':amaj_fft})\n",
    "\n",
    "\n",
    "\n",
    "rwds = rwds.append(emaj_dict,ignore_index=True)\n",
    "rwds = rwds.append(qmaj_dict,ignore_index=True)\n",
    "rwds = rwds.append(amaj_dict,ignore_index=True)\n",
    "\n",
    "#rwds[\"one_hot_encoding\"] = [to1hot(x,len(label_to_chord)) for x in rwds['target']]\n",
    "rwds[\"multi_hot_encoding\"] = [toMultiHot(x,12) for x in rwds['pitch_class']]\n",
    "\n",
    "\n",
    "\n",
    "realworld_x = np.vstack(rwds['fft']).reshape(rwds.shape[0],rwds['fft'].iloc[0].shape[0],1,1).astype(np.float32)\n",
    "#realworld_y = np.vstack(rwds[\"one_hot_encoding\"])\n",
    "realworld_y = np.vstack(rwds[\"multi_hot_encoding\"])\n",
    "\n",
    "\n",
    "tmp1, tmp2 = sess.run([tf.nn.sigmoid(out_y), true_y], \n",
    "                            feed_dict={\n",
    "                                inputs:realworld_x,\n",
    "                                true_y:realworld_y, \n",
    "                                keep_prob1:1.0, keep_prob2:1.0, keep_prob3:1.0})\n",
    "\n",
    "for i in range(tmp1.shape[0]):\n",
    "    #print('Guess vector: '+str(tmp1[i,:]))\n",
    "    print('Guess: '+str(np.where(np.round(tmp1[i,:])==1))+' Reality: '+str(np.where(tmp2[i,:]==1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#any_song_file is originally an mp3 on my hard drive of The Beatles' \"Something\"\n",
    "#I can't upload that, but you can substitute any song of your choosing\n",
    "something, something_sr = librosa.load(any_song_file)\n",
    "\n",
    "something_harmonic, something_percussive = librosa.effects.hpss(something)\n",
    "print(something_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_fft(sample):\n",
    "    try:\n",
    "        S_sample = np.abs((librosa.stft(sample)))**0.5\n",
    "        return S_sample[:,range(0,S_sample.shape[1],9)]\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "tmp = extract_fft(something)\n",
    "print(tmp.shape)\n",
    "print(len(something)/something_sr/60.)\n",
    "print(7881./(float(len(something))/float(something_sr))/5.)\n",
    "print(round(float(len(something))/7881.*4))\n",
    "print('Samples per sec: %g'%(876/3.03/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "something_x = extract_fft(something_harmonic).transpose()\n",
    "something_x = something_x.reshape([something_x.shape[0], something_x.shape[1], 1, 1])\n",
    "something_x.shape\n",
    "\n",
    "#I want to get indices of train_x that are correctly, and incorrectly identified\n",
    "tmp1 = sess.run(tf.round(tf.nn.sigmoid(out_y)), \n",
    "                            feed_dict={\n",
    "                                inputs:something_x,\n",
    "                                #true_y:validation_y, \n",
    "                                keep_prob1:1.0,keep_prob2:1.0,keep_prob3:1.0})\n",
    "\n",
    "\n",
    "\n",
    "print(\"Predictions:\")\n",
    "for i in range(tmp1.shape[0]):\n",
    "    print(np.where(tmp1[i,:]==1))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
